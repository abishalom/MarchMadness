{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import csv\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we clean up the data. We create 2 dataframes, one for winners and one for losers. Each has the same column names (team, score, fgm, fga, fgm3, fga3, ftm, fta, or, dr, ast, stl, blk, pf).\n",
    "<br> We will now use these dataframes to build our representative team vectors for the season, putting together the information from the winners and the losers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = ['team', 'score', 'fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'stl', 'blk', 'pf']\n",
    "imp_attributes = ['team', 'score', 'fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'stl', 'blk']\n",
    "\n",
    "def load_team_data():\n",
    "    reg_season_data_filename = \"RegularSeasonDetailedResults.csv\"\n",
    "    #We load the data into a pandas dataframe.\n",
    "    reg_season_data = pd.read_csv(reg_season_data_filename)\n",
    "    # reg_season_data = reg_season_data.loc[lambda df: df.Season == 2003]\n",
    "    #Took out location and overtimes. Play around with what other stats we want to include. \n",
    "    \n",
    "    w = reg_season_data[['Season'] + ['W' + x for x in imp_attributes]]\n",
    "    win_remap = {'W'+x: x for x in imp_attributes}\n",
    "    winners = w.rename(index = str, columns = win_remap)\n",
    "    los_remap = {'L' + x:x for x in imp_attributes}\n",
    "    l = reg_season_data[['Season']+ ['L' + x for x in imp_attributes]]\n",
    "    losers = l.rename(index=str, columns = los_remap)\n",
    "    \n",
    "    #Know what seasons of data we are dealing with\n",
    "    reg_season_years = reg_season_data['Season'].unique()\n",
    "\n",
    "    #Here, we put all the data (from both winners and losers) into one dataframe. \n",
    "    team_data = pd.concat([winners, losers])\n",
    "\n",
    "    by_reg_season = {}\n",
    "    for year in reg_season_years:\n",
    "        s = team_data[team_data['Season']==year]\n",
    "    #     display(s)\n",
    "        s = s.drop(['Season'], axis = 1)\n",
    "        by_reg_season[year] = s\n",
    "    return by_reg_season, reg_season_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "by_reg_season, reg_season_years = load_team_data()\n",
    "# display(by_reg_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denominate the team vector to be represented as:\n",
    "\\[Score, FG Made, FG Attempted, 3 Pointers Made, 3 Pointers attempted, FT made, FT Attempted, Off Rebounds, Def Rebounds, Assists, Steals, Blocks\\]. (Later in the code we add the team's seed to their vectors)\n",
    "<br> <br>\n",
    "In this following cell we aggregate the data and calculate each team's average statistics for each year. To store this, we keep 1 dataframe (indexed by team) for each year of data. The pointers to these objects are stored in our dictionary, which serves as a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_vector_store(by_reg_season, reg_season_years):\n",
    "    # Create dictionary mapping year-> Dataframe of teams within that year.\n",
    "    team_vector_store = {}\n",
    "\n",
    "    for year in reg_season_years:\n",
    "        t = by_reg_season[year].groupby('team')[imp_attributes[1:]].mean()\n",
    "        team_vector_store[year] = t.apply(lambda x: x, axis = 0)\n",
    "    \n",
    "    return team_vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_vector_store = get_team_vector_store(by_reg_season, reg_season_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to normalize the data. This could be used at whatever point we want. If we want a standardn normal distribution, set standard = True, otherwise we let standard=False for min/max normalization.\n",
    "\n",
    "We test using both methods of normalization to see which gave us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(team_vector_store, standard = False):\n",
    "    \"\"\"\n",
    "    Function to normalize the data. \n",
    "    IMPORTANT: Modifies the argument passed in.\n",
    "    Set standard to True if normalizing to standard Gaussian,\n",
    "    False for min/max normalization. \n",
    "    \"\"\"\n",
    "    for y in reg_season_years:\n",
    "        if standard:\n",
    "            team_vector_store[y] = team_vector_store[y].apply(lambda x: (x - x.mean())/x.std(), axis = 0)\n",
    "        else:\n",
    "            team_vector_store[y] = team_vector_store[y].apply(lambda x: x/x.max(), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a matchup to be the difference between the vectors of team1 and team2. If team 1 wins, we classify the matchup as a 1, while we classify it as a 0 if team2 wins. Below is a quick example of how it could work. However, we must now add each team's seed to their representative vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2003\n",
    "m = team_vector_store[year]\n",
    "matchup_1 = (m.loc[1102] - m.loc[1103])\n",
    "matchup_1['class'] = 0\n",
    "# display(matchup_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we modify each season's team vector dataframe by adding each team's NCAA tournament seed. This will allow us to use this as a feature when outputting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed_data(filename):\n",
    "    tourney_seeds_data = pd.read_csv(filename)\n",
    "    tourney_seeds_data = tourney_seeds_data.query('Season >= 2003')\n",
    "    tourney_seeds_data = tourney_seeds_data.reset_index(drop=True)\n",
    "    tourney_seeds_data.rename(str.lower, axis='columns', inplace = True)\n",
    "    tourney_seeds_data['seed'] = tourney_seeds_data['seed'].str.extract('(\\d+)', expand = False).astype(int)\n",
    "    return tourney_seeds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_seeds_data = load_seed_data(\"TourneySeeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vector_store(team_vector_store, tourney_seeds_data):\n",
    "    #Joins the team vector store with the seed information.\n",
    "    #Allows us to consider seed as an attribute of the team.\n",
    "    #Eliminates the teams not in the tournament.\n",
    "    for s in reg_season_years:\n",
    "        relevant_data = tourney_seeds_data.query('season == {}'.format(s))\n",
    "        relevant_data = relevant_data.drop('season', axis = 1)\n",
    "        m = team_vector_store[s]\n",
    "        m = m.merge(relevant_data, how = 'outer', left_index = True, right_on = 'team')\n",
    "        m.set_index('team', inplace=True)\n",
    "        m = m[pd.notnull(m['seed'])]\n",
    "        team_vector_store[s] = m\n",
    "    return team_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_vector_store = update_vector_store(team_vector_store, tourney_seeds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(team_vector_store, standard = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must use our team vectors along with the matchups each season to build our feature vectors, which we will use for classification. A 1 will signify that the 1st team won, while a 0 means the 2nd team won.\n",
    "\n",
    "Our NCAA tournament data is in the same format as the regular season one. In order to have a combination of 1s and 0s, we will generate 2 different vectors for each matchup, where the first has team_1 = winning_team while the second has team_1 = losing_team. Because of the way our data is labeled, if we do not do this then all of our vectors will have classification 1 or 0, which won't allow our model to actually learn a decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(team_vector_store, classification = True):\n",
    "    \"\"\"\n",
    "    Function to generate the training/testing data.\n",
    "    Based on team_vector_store and the tourney detailed results.\n",
    "    If classification = True, generates for binary classification.\n",
    "    If classification = False, generates for regression.\n",
    "    \n",
    "    Returns training_data\n",
    "    \"\"\"\n",
    "    tourney_game_data = pd.read_csv(\"TourneyDetailedResults.csv\")\n",
    "    tourney_game_data = tourney_game_data[['Season', 'Wteam', 'Lteam', 'Wscore', 'Lscore']]\n",
    "\n",
    "\n",
    "    col_names = list(team_vector_store[2003]) + ['class']\n",
    "\n",
    "    train_matchup_data = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    for index, row in tourney_game_data.iterrows():\n",
    "        season, wteam, lteam, wscore, lscore = row\n",
    "        \n",
    "        #Winner is team 1, loser is team 2\n",
    "        game_vector_1 = team_vector_store[season].loc[wteam] - team_vector_store[season].loc[lteam]\n",
    "        if classification:\n",
    "            game_vector_1['class'] = 1\n",
    "        else:\n",
    "            game_vector_1['class'] = wscore - lscore\n",
    "        \n",
    "        #Loser is team 1, winner is team 2\n",
    "        game_vector_2 = team_vector_store[season].loc[lteam] - team_vector_store[season].loc[wteam]\n",
    "        if classification:\n",
    "            game_vector_2['class'] = 0\n",
    "        else:\n",
    "            game_vector_2['class'] = lscore - wscore\n",
    "        \n",
    "        train_matchup_data = train_matchup_data.append([game_vector_1, game_vector_2], ignore_index = True)\n",
    "        \n",
    "    return train_matchup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matchup_data_class = generate_training_data(team_vector_store, classification = True)\n",
    "train_matchup_data_reg = generate_training_data(team_vector_store, classification = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our training data built up. The X data will just be the rows except ('class'), while the Y data will simply be the column 'class'.\n",
    "\n",
    "Below, we define functions to build 2 different kinds of simple neural networks:\n",
    "1. A binary classification network: \n",
    "    - Takes matchup vectors (team_1 - team_2) as inputs, and outputs a binary digit prediction (1 if team_1 wins, 0 if team_2 wins).\n",
    "2. A regression network: \n",
    "    - Takes matchup vectors (team_1 - team_2) as inputs, and outputs a scalar prediction for the difference between scores (team_1_score - team_2_score).\n",
    "    - We then use these predicted scores to predict the matchup victor (1 if predicted score > 0, and 0 otherwise)\n",
    "    \n",
    "In order to evaluate the regression network's predictions, we define a prediction_percentage function which returns the percentage of games in which the victor is predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_percentage(y_true, y_pred):\n",
    "    y_true_sign = tf.sign(y_true)\n",
    "    y_pred_sign = tf.sign(y_pred)\n",
    "    mults = tf.multiply(y_true_sign, y_pred_sign)\n",
    "    return K.mean(mults+1)/2\n",
    "\n",
    "def build_keras_model(input_shape, num_units, num_layers, classification = True):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(units = num_units, \n",
    "                    input_dim = input_shape[1], \n",
    "                    activation = 'relu',\n",
    "                    kernel_regularizer = l1_l2(0, 0.001)))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dense(num_units, \n",
    "                        activation = 'relu',\n",
    "                        kernel_regularizer = l1_l2(0, 0.001)))\n",
    "#         model.add(Dropout(0.25))\n",
    "    \n",
    "    if classification:\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = Adam(), metrics = ['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss = 'mae', optimizer = Adam(), metrics = [pred_percentage])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, X_train, Y_train, X_test, Y_test, epochs):\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = 1)\n",
    "    loss = model.evaluate(X_test, Y_test, batch_size = 1)\n",
    "    print(\"Loss was: {} and accuracy was: {}\".format(loss[0], loss[1]))\n",
    "    return loss[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use cross-validation (with 10 folds) to test the performance of our binary classification neural network. This process splits the data into 10 randomized chunks. Then, it uses $\\frac{9}{10}$ chunks to train the network, while using the final chunk to test the predictions. After this, the average accuracy is printed below, and serves as a proxy for the performance of this neural network on unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class = train_matchup_data_class.drop(['class'], axis = 1)\n",
    "input_shape = X_train_class.shape\n",
    "Y_train_class = train_matchup_data_class['class']\n",
    "\n",
    "num_units = 15\n",
    "num_layers = 3\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(Y_train_class, n_folds = n_folds, shuffle = True)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = build_keras_model(input_shape, num_units, num_layers, classification = True)\n",
    "    accuracy = train_and_evaluate(model, X_train_class.iloc[train], \n",
    "                       Y_train_class.iloc[train], \n",
    "                       X_train_class.iloc[test], \n",
    "                       Y_train_class.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"The average accuracy of predictions was: {}\".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the performance of our regression neural network. As above, we use cross validation (with 10 folds) in order to test our network on our given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg = train_matchup_data_reg.drop(['class'], axis = 1)\n",
    "input_shape = X_train_reg.shape\n",
    "Y_train_reg = train_matchup_data_reg['class']\n",
    "\n",
    "num_units = 15\n",
    "num_layers = 3\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(Y_train_reg, n_folds = n_folds, shuffle = True)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = build_keras_model(input_shape, num_units, num_layers, classification = False)\n",
    "    accuracy = train_and_evaluate(model, X_train_reg.iloc[train], \n",
    "                       Y_train_reg.iloc[train], \n",
    "                       X_train_reg.iloc[test], \n",
    "                       Y_train_reg.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"The average accuracy of predictions was: {}\".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final neural network we will test is a bit more complex. Instead of taking a matchup vector as input, this neural network is simply provided the two team vectors. This allows the neural network to learn its own definition for a \"matchup\", instead of having to use ours. As with model #2, this is a regression network, which means its predictions represent the difference between the scores of the teams. \n",
    "\n",
    "In order to build this, we have to build our training set in a different way: building two different training sets in conjunction (one for team_1s and the other for team_2s), and then the labels in a third pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Generates data sets for inputs into the third neural network.\n",
    "    \"\"\"\n",
    "    tourney_game_data = pd.read_csv(\"TourneyDetailedResults.csv\")\n",
    "    tourney_game_data = tourney_game_data[['Season', 'Wteam', 'Lteam', 'Wscore', 'Lscore']]\n",
    "\n",
    "    # display(tourney_game_data)\n",
    "\n",
    "    col_names = list(team_vector_store[2003])\n",
    "    # print(list(col_names))\n",
    "\n",
    "    team_ones = pd.DataFrame(columns = col_names)\n",
    "    team_twos = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    one_minus_two = []\n",
    "    \n",
    "    for index, row in tourney_game_data.iterrows():\n",
    "        season, wteam, lteam, wscore, lscore = row\n",
    "        \n",
    "        win_vector = team_vector_store[season].loc[wteam]\n",
    "        lose_vector = team_vector_store[season].loc[lteam]\n",
    "        \n",
    "        #Win minus lose\n",
    "        team_ones = team_ones.append(win_vector)\n",
    "        team_twos = team_twos.append(lose_vector)\n",
    "        one_minus_two.append(wscore-lscore)\n",
    "    \n",
    "        #Lose minus win\n",
    "        team_ones = team_ones.append(lose_vector)\n",
    "        team_twos = team_twos.append(win_vector)\n",
    "        one_minus_two.append(lscore-wscore)\n",
    "    \n",
    "    one_minus_two = pd.Series(one_minus_two)\n",
    "    return team_ones, team_twos, one_minus_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>fgm</th>\n",
       "      <th>fga</th>\n",
       "      <th>fgm3</th>\n",
       "      <th>fga3</th>\n",
       "      <th>ftm</th>\n",
       "      <th>fta</th>\n",
       "      <th>or</th>\n",
       "      <th>dr</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>0.835622</td>\n",
       "      <td>0.804029</td>\n",
       "      <td>0.864243</td>\n",
       "      <td>0.684798</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.758773</td>\n",
       "      <td>0.828541</td>\n",
       "      <td>0.689916</td>\n",
       "      <td>0.648954</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>0.854317</td>\n",
       "      <td>0.815705</td>\n",
       "      <td>0.841014</td>\n",
       "      <td>0.626761</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>0.868503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813834</td>\n",
       "      <td>0.886736</td>\n",
       "      <td>0.751607</td>\n",
       "      <td>0.590601</td>\n",
       "      <td>0.288793</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743209</td>\n",
       "      <td>0.708403</td>\n",
       "      <td>0.875277</td>\n",
       "      <td>0.890736</td>\n",
       "      <td>0.938190</td>\n",
       "      <td>0.988384</td>\n",
       "      <td>0.933837</td>\n",
       "      <td>0.777049</td>\n",
       "      <td>0.544951</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0.795560</td>\n",
       "      <td>0.818813</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>0.557309</td>\n",
       "      <td>0.546450</td>\n",
       "      <td>0.641997</td>\n",
       "      <td>0.696617</td>\n",
       "      <td>0.801401</td>\n",
       "      <td>0.919779</td>\n",
       "      <td>0.751972</td>\n",
       "      <td>0.629960</td>\n",
       "      <td>0.383472</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>0.891465</td>\n",
       "      <td>0.897283</td>\n",
       "      <td>0.865817</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.444219</td>\n",
       "      <td>0.876076</td>\n",
       "      <td>0.933737</td>\n",
       "      <td>0.846160</td>\n",
       "      <td>0.833473</td>\n",
       "      <td>0.823154</td>\n",
       "      <td>0.478010</td>\n",
       "      <td>0.548454</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>0.874469</td>\n",
       "      <td>0.866577</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.739437</td>\n",
       "      <td>0.708316</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.815792</td>\n",
       "      <td>0.869605</td>\n",
       "      <td>0.928409</td>\n",
       "      <td>0.879734</td>\n",
       "      <td>0.677445</td>\n",
       "      <td>0.655470</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>0.931121</td>\n",
       "      <td>0.877950</td>\n",
       "      <td>0.801799</td>\n",
       "      <td>0.721224</td>\n",
       "      <td>0.632860</td>\n",
       "      <td>0.962134</td>\n",
       "      <td>0.896879</td>\n",
       "      <td>0.654335</td>\n",
       "      <td>0.832240</td>\n",
       "      <td>0.826804</td>\n",
       "      <td>0.652120</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>0.929919</td>\n",
       "      <td>0.946425</td>\n",
       "      <td>0.874308</td>\n",
       "      <td>0.841869</td>\n",
       "      <td>0.722995</td>\n",
       "      <td>0.692745</td>\n",
       "      <td>0.713669</td>\n",
       "      <td>0.672420</td>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.890187</td>\n",
       "      <td>0.770591</td>\n",
       "      <td>0.576019</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>0.874064</td>\n",
       "      <td>0.901832</td>\n",
       "      <td>0.893628</td>\n",
       "      <td>0.677513</td>\n",
       "      <td>0.601217</td>\n",
       "      <td>0.667814</td>\n",
       "      <td>0.695389</td>\n",
       "      <td>0.694831</td>\n",
       "      <td>0.871694</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.601470</td>\n",
       "      <td>0.361177</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0.849623</td>\n",
       "      <td>0.802513</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.841549</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.786976</td>\n",
       "      <td>0.729216</td>\n",
       "      <td>0.601619</td>\n",
       "      <td>0.787812</td>\n",
       "      <td>0.776307</td>\n",
       "      <td>0.713005</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>0.939201</td>\n",
       "      <td>0.974009</td>\n",
       "      <td>0.946522</td>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.743718</td>\n",
       "      <td>0.787411</td>\n",
       "      <td>0.912730</td>\n",
       "      <td>0.997578</td>\n",
       "      <td>0.827473</td>\n",
       "      <td>0.544699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>0.850228</td>\n",
       "      <td>0.792583</td>\n",
       "      <td>0.780014</td>\n",
       "      <td>0.654248</td>\n",
       "      <td>0.569260</td>\n",
       "      <td>0.908112</td>\n",
       "      <td>0.860854</td>\n",
       "      <td>0.671936</td>\n",
       "      <td>0.873126</td>\n",
       "      <td>0.710287</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0.961889</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>0.918116</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.727059</td>\n",
       "      <td>0.993287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850920</td>\n",
       "      <td>0.825952</td>\n",
       "      <td>0.732199</td>\n",
       "      <td>0.780328</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.868399</td>\n",
       "      <td>0.870671</td>\n",
       "      <td>0.792826</td>\n",
       "      <td>0.443662</td>\n",
       "      <td>0.408235</td>\n",
       "      <td>0.848537</td>\n",
       "      <td>0.864608</td>\n",
       "      <td>0.667550</td>\n",
       "      <td>0.839062</td>\n",
       "      <td>0.820416</td>\n",
       "      <td>0.489617</td>\n",
       "      <td>0.547414</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>0.904361</td>\n",
       "      <td>0.859607</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.756474</td>\n",
       "      <td>0.672865</td>\n",
       "      <td>0.887180</td>\n",
       "      <td>0.878094</td>\n",
       "      <td>0.737734</td>\n",
       "      <td>0.905421</td>\n",
       "      <td>0.833222</td>\n",
       "      <td>0.624855</td>\n",
       "      <td>0.454672</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>0.790025</td>\n",
       "      <td>0.755006</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.705483</td>\n",
       "      <td>0.688235</td>\n",
       "      <td>0.741579</td>\n",
       "      <td>0.767306</td>\n",
       "      <td>0.750552</td>\n",
       "      <td>0.836423</td>\n",
       "      <td>0.650284</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.549569</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>0.885219</td>\n",
       "      <td>0.893757</td>\n",
       "      <td>0.849130</td>\n",
       "      <td>0.785211</td>\n",
       "      <td>0.711765</td>\n",
       "      <td>0.688812</td>\n",
       "      <td>0.680523</td>\n",
       "      <td>0.601619</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.966856</td>\n",
       "      <td>0.642623</td>\n",
       "      <td>0.400862</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.847905</td>\n",
       "      <td>0.850701</td>\n",
       "      <td>0.814403</td>\n",
       "      <td>0.742315</td>\n",
       "      <td>0.697185</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.751691</td>\n",
       "      <td>0.831603</td>\n",
       "      <td>0.751265</td>\n",
       "      <td>0.648546</td>\n",
       "      <td>0.458843</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>0.954457</td>\n",
       "      <td>0.997095</td>\n",
       "      <td>0.948043</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.498824</td>\n",
       "      <td>0.801951</td>\n",
       "      <td>0.859857</td>\n",
       "      <td>0.883885</td>\n",
       "      <td>0.961822</td>\n",
       "      <td>0.885696</td>\n",
       "      <td>0.930273</td>\n",
       "      <td>0.633621</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>0.774127</td>\n",
       "      <td>0.781625</td>\n",
       "      <td>0.790290</td>\n",
       "      <td>0.426056</td>\n",
       "      <td>0.428235</td>\n",
       "      <td>0.725416</td>\n",
       "      <td>0.720903</td>\n",
       "      <td>0.780868</td>\n",
       "      <td>0.799731</td>\n",
       "      <td>0.735728</td>\n",
       "      <td>0.465137</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.919950</td>\n",
       "      <td>0.897527</td>\n",
       "      <td>0.855978</td>\n",
       "      <td>0.611167</td>\n",
       "      <td>0.538235</td>\n",
       "      <td>0.907364</td>\n",
       "      <td>0.841110</td>\n",
       "      <td>0.810155</td>\n",
       "      <td>0.860685</td>\n",
       "      <td>0.863894</td>\n",
       "      <td>0.550820</td>\n",
       "      <td>0.471059</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>0.813770</td>\n",
       "      <td>0.798343</td>\n",
       "      <td>0.802324</td>\n",
       "      <td>0.560952</td>\n",
       "      <td>0.520892</td>\n",
       "      <td>0.779690</td>\n",
       "      <td>0.788762</td>\n",
       "      <td>0.745985</td>\n",
       "      <td>0.869228</td>\n",
       "      <td>0.815853</td>\n",
       "      <td>0.731261</td>\n",
       "      <td>0.579667</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0.842279</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.900771</td>\n",
       "      <td>0.790550</td>\n",
       "      <td>0.740038</td>\n",
       "      <td>0.597357</td>\n",
       "      <td>0.647077</td>\n",
       "      <td>0.833440</td>\n",
       "      <td>0.926182</td>\n",
       "      <td>0.727361</td>\n",
       "      <td>0.524167</td>\n",
       "      <td>0.558954</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>0.874658</td>\n",
       "      <td>0.902552</td>\n",
       "      <td>0.880580</td>\n",
       "      <td>0.644366</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.683821</td>\n",
       "      <td>0.691211</td>\n",
       "      <td>0.669610</td>\n",
       "      <td>0.803306</td>\n",
       "      <td>0.718084</td>\n",
       "      <td>0.731366</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0.939945</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.923352</td>\n",
       "      <td>0.875738</td>\n",
       "      <td>0.768501</td>\n",
       "      <td>0.866248</td>\n",
       "      <td>0.813731</td>\n",
       "      <td>0.703838</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.894689</td>\n",
       "      <td>0.684082</td>\n",
       "      <td>0.729978</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0.904778</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.877029</td>\n",
       "      <td>0.820423</td>\n",
       "      <td>0.769412</td>\n",
       "      <td>0.738726</td>\n",
       "      <td>0.763658</td>\n",
       "      <td>0.712877</td>\n",
       "      <td>0.822376</td>\n",
       "      <td>0.905104</td>\n",
       "      <td>0.807869</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.835149</td>\n",
       "      <td>0.833294</td>\n",
       "      <td>0.860290</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.669412</td>\n",
       "      <td>0.657200</td>\n",
       "      <td>0.662708</td>\n",
       "      <td>0.749963</td>\n",
       "      <td>0.892695</td>\n",
       "      <td>0.749842</td>\n",
       "      <td>0.636503</td>\n",
       "      <td>0.487069</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>0.854317</td>\n",
       "      <td>0.817903</td>\n",
       "      <td>0.827319</td>\n",
       "      <td>0.545775</td>\n",
       "      <td>0.545882</td>\n",
       "      <td>0.900115</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.801472</td>\n",
       "      <td>0.867666</td>\n",
       "      <td>0.543415</td>\n",
       "      <td>0.734426</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>0.847957</td>\n",
       "      <td>0.845777</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>0.732622</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.700405</td>\n",
       "      <td>0.740173</td>\n",
       "      <td>0.849391</td>\n",
       "      <td>0.898501</td>\n",
       "      <td>0.775169</td>\n",
       "      <td>0.506399</td>\n",
       "      <td>0.433815</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>0.876362</td>\n",
       "      <td>0.831567</td>\n",
       "      <td>0.814674</td>\n",
       "      <td>0.731891</td>\n",
       "      <td>0.636555</td>\n",
       "      <td>0.864581</td>\n",
       "      <td>0.937818</td>\n",
       "      <td>0.713024</td>\n",
       "      <td>0.942412</td>\n",
       "      <td>0.867675</td>\n",
       "      <td>0.495082</td>\n",
       "      <td>0.443350</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>0.965040</td>\n",
       "      <td>0.902654</td>\n",
       "      <td>0.882544</td>\n",
       "      <td>0.778080</td>\n",
       "      <td>0.715997</td>\n",
       "      <td>0.835726</td>\n",
       "      <td>0.808979</td>\n",
       "      <td>0.662746</td>\n",
       "      <td>0.906257</td>\n",
       "      <td>0.751723</td>\n",
       "      <td>0.679756</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0.907695</td>\n",
       "      <td>0.849699</td>\n",
       "      <td>0.836394</td>\n",
       "      <td>0.742515</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.777961</td>\n",
       "      <td>0.699339</td>\n",
       "      <td>0.579367</td>\n",
       "      <td>0.884888</td>\n",
       "      <td>0.684013</td>\n",
       "      <td>0.578402</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.966280</td>\n",
       "      <td>0.885772</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.774671</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.673637</td>\n",
       "      <td>0.950435</td>\n",
       "      <td>0.706813</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>0.912268</td>\n",
       "      <td>0.854415</td>\n",
       "      <td>0.908406</td>\n",
       "      <td>0.735470</td>\n",
       "      <td>0.815321</td>\n",
       "      <td>0.786378</td>\n",
       "      <td>0.798942</td>\n",
       "      <td>0.805915</td>\n",
       "      <td>0.881592</td>\n",
       "      <td>0.821173</td>\n",
       "      <td>0.695266</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>0.943918</td>\n",
       "      <td>0.875448</td>\n",
       "      <td>0.902043</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.743058</td>\n",
       "      <td>0.891547</td>\n",
       "      <td>0.852589</td>\n",
       "      <td>0.738923</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.765107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0.978878</td>\n",
       "      <td>0.879334</td>\n",
       "      <td>0.915506</td>\n",
       "      <td>0.888405</td>\n",
       "      <td>0.891896</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.769394</td>\n",
       "      <td>0.815016</td>\n",
       "      <td>0.655915</td>\n",
       "      <td>0.643177</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.924993</td>\n",
       "      <td>0.853472</td>\n",
       "      <td>0.872358</td>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.926949</td>\n",
       "      <td>0.798762</td>\n",
       "      <td>0.702646</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.891542</td>\n",
       "      <td>0.785408</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>0.586397</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>0.908822</td>\n",
       "      <td>0.838677</td>\n",
       "      <td>0.840703</td>\n",
       "      <td>0.655689</td>\n",
       "      <td>0.696512</td>\n",
       "      <td>0.866776</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>0.608826</td>\n",
       "      <td>0.847886</td>\n",
       "      <td>0.623212</td>\n",
       "      <td>0.609837</td>\n",
       "      <td>0.623047</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.988614</td>\n",
       "      <td>0.987387</td>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.515675</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.881701</td>\n",
       "      <td>0.911443</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.753217</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>0.988813</td>\n",
       "      <td>0.935872</td>\n",
       "      <td>0.892409</td>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.886047</td>\n",
       "      <td>0.738487</td>\n",
       "      <td>0.698214</td>\n",
       "      <td>0.756123</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.776735</td>\n",
       "      <td>0.697855</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0.909949</td>\n",
       "      <td>0.870741</td>\n",
       "      <td>0.882834</td>\n",
       "      <td>0.703593</td>\n",
       "      <td>0.740698</td>\n",
       "      <td>0.740132</td>\n",
       "      <td>0.688095</td>\n",
       "      <td>0.685421</td>\n",
       "      <td>0.851057</td>\n",
       "      <td>0.655132</td>\n",
       "      <td>0.562685</td>\n",
       "      <td>0.671387</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0.827704</td>\n",
       "      <td>0.752505</td>\n",
       "      <td>0.836873</td>\n",
       "      <td>0.631737</td>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.807566</td>\n",
       "      <td>0.780291</td>\n",
       "      <td>0.718808</td>\n",
       "      <td>0.793968</td>\n",
       "      <td>0.563931</td>\n",
       "      <td>0.590976</td>\n",
       "      <td>0.547852</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0.843852</td>\n",
       "      <td>0.767535</td>\n",
       "      <td>0.859854</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.715461</td>\n",
       "      <td>0.716204</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>0.802425</td>\n",
       "      <td>0.684013</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>0.955390</td>\n",
       "      <td>0.897796</td>\n",
       "      <td>0.883792</td>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.769767</td>\n",
       "      <td>0.827303</td>\n",
       "      <td>0.744312</td>\n",
       "      <td>0.657925</td>\n",
       "      <td>0.977923</td>\n",
       "      <td>0.674893</td>\n",
       "      <td>0.502959</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.845594</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.574850</td>\n",
       "      <td>0.551374</td>\n",
       "      <td>0.679426</td>\n",
       "      <td>0.616001</td>\n",
       "      <td>0.563715</td>\n",
       "      <td>0.775034</td>\n",
       "      <td>0.697186</td>\n",
       "      <td>0.551730</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.983556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952733</td>\n",
       "      <td>0.793413</td>\n",
       "      <td>0.810465</td>\n",
       "      <td>0.588816</td>\n",
       "      <td>0.570040</td>\n",
       "      <td>0.571511</td>\n",
       "      <td>0.912376</td>\n",
       "      <td>0.804095</td>\n",
       "      <td>0.619268</td>\n",
       "      <td>0.725098</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.966280</td>\n",
       "      <td>0.885772</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.774671</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.673637</td>\n",
       "      <td>0.950435</td>\n",
       "      <td>0.706813</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>0.943918</td>\n",
       "      <td>0.875448</td>\n",
       "      <td>0.902043</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.743058</td>\n",
       "      <td>0.891547</td>\n",
       "      <td>0.852589</td>\n",
       "      <td>0.738923</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.765107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.924993</td>\n",
       "      <td>0.853472</td>\n",
       "      <td>0.872358</td>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.926949</td>\n",
       "      <td>0.798762</td>\n",
       "      <td>0.702646</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.891542</td>\n",
       "      <td>0.785408</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>0.586397</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>0.965040</td>\n",
       "      <td>0.902654</td>\n",
       "      <td>0.882544</td>\n",
       "      <td>0.778080</td>\n",
       "      <td>0.715997</td>\n",
       "      <td>0.835726</td>\n",
       "      <td>0.808979</td>\n",
       "      <td>0.662746</td>\n",
       "      <td>0.906257</td>\n",
       "      <td>0.751723</td>\n",
       "      <td>0.679756</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.988614</td>\n",
       "      <td>0.987387</td>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.515675</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.881701</td>\n",
       "      <td>0.911443</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.753217</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0.909949</td>\n",
       "      <td>0.870741</td>\n",
       "      <td>0.882834</td>\n",
       "      <td>0.703593</td>\n",
       "      <td>0.740698</td>\n",
       "      <td>0.740132</td>\n",
       "      <td>0.688095</td>\n",
       "      <td>0.685421</td>\n",
       "      <td>0.851057</td>\n",
       "      <td>0.655132</td>\n",
       "      <td>0.562685</td>\n",
       "      <td>0.671387</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0.843852</td>\n",
       "      <td>0.767535</td>\n",
       "      <td>0.859854</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.715461</td>\n",
       "      <td>0.716204</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>0.802425</td>\n",
       "      <td>0.684013</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.845594</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.574850</td>\n",
       "      <td>0.551374</td>\n",
       "      <td>0.679426</td>\n",
       "      <td>0.616001</td>\n",
       "      <td>0.563715</td>\n",
       "      <td>0.775034</td>\n",
       "      <td>0.697186</td>\n",
       "      <td>0.551730</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.988614</td>\n",
       "      <td>0.987387</td>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.515675</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.881701</td>\n",
       "      <td>0.911443</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.753217</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0.843852</td>\n",
       "      <td>0.767535</td>\n",
       "      <td>0.859854</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.715461</td>\n",
       "      <td>0.716204</td>\n",
       "      <td>0.742375</td>\n",
       "      <td>0.802425</td>\n",
       "      <td>0.684013</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.924993</td>\n",
       "      <td>0.853472</td>\n",
       "      <td>0.872358</td>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.926949</td>\n",
       "      <td>0.798762</td>\n",
       "      <td>0.702646</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.891542</td>\n",
       "      <td>0.785408</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>0.586397</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.966280</td>\n",
       "      <td>0.885772</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.774671</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.673637</td>\n",
       "      <td>0.950435</td>\n",
       "      <td>0.706813</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>0.924993</td>\n",
       "      <td>0.853472</td>\n",
       "      <td>0.872358</td>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.926949</td>\n",
       "      <td>0.798762</td>\n",
       "      <td>0.702646</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.891542</td>\n",
       "      <td>0.785408</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>0.586397</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.988614</td>\n",
       "      <td>0.987387</td>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.515675</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.881701</td>\n",
       "      <td>0.911443</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.753217</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1426 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         score       fgm       fga      fgm3      fga3       ftm       fta  \\\n",
       "1421  0.835622  0.804029  0.864243  0.684798  0.635294  0.796902  0.745761   \n",
       "1411  0.854317  0.815705  0.841014  0.626761  0.652941  0.868503  1.000000   \n",
       "1112  1.000000  1.000000  1.000000  0.743209  0.708403  0.875277  0.890736   \n",
       "1436  0.795560  0.818813  0.850075  0.557309  0.546450  0.641997  0.696617   \n",
       "1113  0.891465  0.897283  0.865817  0.422535  0.444219  0.876076  0.933737   \n",
       "1272  0.874469  0.866577  0.913043  0.739437  0.708316  0.746988  0.815792   \n",
       "1141  0.931121  0.877950  0.801799  0.721224  0.632860  0.962134  0.896879   \n",
       "1166  0.929919  0.946425  0.874308  0.841869  0.722995  0.692745  0.713669   \n",
       "1143  0.874064  0.901832  0.893628  0.677513  0.601217  0.667814  0.695389   \n",
       "1301  0.849623  0.802513  0.811594  0.841549  0.794118  0.786976  0.729216   \n",
       "1163  0.939201  0.974009  0.946522  0.640845  0.554118  0.743718  0.787411   \n",
       "1140  0.850228  0.792583  0.780014  0.654248  0.569260  0.908112  0.860854   \n",
       "1181  0.961889  0.902552  0.918116  0.774648  0.727059  0.993287  1.000000   \n",
       "1161  0.868399  0.870671  0.792826  0.443662  0.408235  0.848537  0.864608   \n",
       "1211  0.904361  0.859607  0.843829  0.756474  0.672865  0.887180  0.878094   \n",
       "1153  0.790025  0.755006  0.862500  0.705483  0.688235  0.741579  0.767306   \n",
       "1228  0.885219  0.893757  0.849130  0.785211  0.711765  0.688812  0.680523   \n",
       "1443  0.857800  0.847905  0.850701  0.814403  0.742315  0.697185  0.751667   \n",
       "1242  0.954457  0.997095  0.948043  0.507042  0.498824  0.801951  0.859857   \n",
       "1429  0.774127  0.781625  0.790290  0.426056  0.428235  0.725416  0.720903   \n",
       "1266  0.919950  0.897527  0.855978  0.611167  0.538235  0.907364  0.841110   \n",
       "1221  0.813770  0.798343  0.802324  0.560952  0.520892  0.779690  0.788762   \n",
       "1281  0.842279  0.862799  0.900771  0.790550  0.740038  0.597357  0.647077   \n",
       "1356  0.874658  0.902552  0.880580  0.644366  0.558824  0.683821  0.691211   \n",
       "1323  0.939945  0.897906  0.923352  0.875738  0.768501  0.866248  0.813731   \n",
       "1454  0.904778  0.899254  0.877029  0.820423  0.769412  0.738726  0.763658   \n",
       "1328  0.835149  0.833294  0.860290  0.788732  0.669412  0.657200  0.662708   \n",
       "1354  0.854317  0.817903  0.827319  0.545775  0.545882  0.900115  0.919240   \n",
       "1390  0.847957  0.845777  0.883100  0.732622  0.684250  0.700405  0.740173   \n",
       "1360  0.876362  0.831567  0.814674  0.731891  0.636555  0.864581  0.937818   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1242  0.965040  0.902654  0.882544  0.778080  0.715997  0.835726  0.808979   \n",
       "1268  0.907695  0.849699  0.836394  0.742515  0.767442  0.777961  0.699339   \n",
       "1328  0.966280  0.885772  0.923050  1.000000  0.911628  0.774671  0.727447   \n",
       "1401  0.912268  0.854415  0.908406  0.735470  0.815321  0.786378  0.798942   \n",
       "1332  0.943918  0.875448  0.902043  0.670659  0.743058  0.891547  0.852589   \n",
       "1181  0.978878  0.879334  0.915506  0.888405  0.891896  0.912281  0.863492   \n",
       "1437  0.924993  0.853472  0.872358  0.820007  0.926949  0.798762  0.702646   \n",
       "1274  0.908822  0.838677  0.840703  0.655689  0.696512  0.866776  0.789286   \n",
       "1314  0.988614  0.987387  0.985458  0.515675  0.638030  0.804954  0.746032   \n",
       "1231  0.988813  0.935872  0.892409  0.946108  0.886047  0.738487  0.698214   \n",
       "1323  0.909949  0.870741  0.882834  0.703593  0.740698  0.740132  0.688095   \n",
       "1458  0.827704  0.752505  0.836873  0.631737  0.686047  0.807566  0.780291   \n",
       "1393  0.843852  0.767535  0.859854  0.838323  0.902326  0.715461  0.716204   \n",
       "1211  0.955390  0.897796  0.883792  0.745509  0.769767  0.827303  0.744312   \n",
       "1438  0.845594  0.824923  0.809192  0.574850  0.551374  0.679426  0.616001   \n",
       "1235  0.983556  1.000000  0.952733  0.793413  0.810465  0.588816  0.570040   \n",
       "1328  0.966280  0.885772  0.923050  1.000000  0.911628  0.774671  0.727447   \n",
       "1332  0.943918  0.875448  0.902043  0.670659  0.743058  0.891547  0.852589   \n",
       "1437  0.924993  0.853472  0.872358  0.820007  0.926949  0.798762  0.702646   \n",
       "1242  0.965040  0.902654  0.882544  0.778080  0.715997  0.835726  0.808979   \n",
       "1314  0.988614  0.987387  0.985458  0.515675  0.638030  0.804954  0.746032   \n",
       "1323  0.909949  0.870741  0.882834  0.703593  0.740698  0.740132  0.688095   \n",
       "1393  0.843852  0.767535  0.859854  0.838323  0.902326  0.715461  0.716204   \n",
       "1438  0.845594  0.824923  0.809192  0.574850  0.551374  0.679426  0.616001   \n",
       "1314  0.988614  0.987387  0.985458  0.515675  0.638030  0.804954  0.746032   \n",
       "1393  0.843852  0.767535  0.859854  0.838323  0.902326  0.715461  0.716204   \n",
       "1437  0.924993  0.853472  0.872358  0.820007  0.926949  0.798762  0.702646   \n",
       "1328  0.966280  0.885772  0.923050  1.000000  0.911628  0.774671  0.727447   \n",
       "1437  0.924993  0.853472  0.872358  0.820007  0.926949  0.798762  0.702646   \n",
       "1314  0.988614  0.987387  0.985458  0.515675  0.638030  0.804954  0.746032   \n",
       "\n",
       "            or        dr       ast       stl       blk    seed  \n",
       "1421  0.758773  0.828541  0.689916  0.648954  0.387931  1.0000  \n",
       "1411  0.813834  0.886736  0.751607  0.590601  0.288793  1.0000  \n",
       "1112  0.938190  0.988384  0.933837  0.777049  0.544951  0.0625  \n",
       "1436  0.801401  0.919779  0.751972  0.629960  0.383472  1.0000  \n",
       "1113  0.846160  0.833473  0.823154  0.478010  0.548454  0.6250  \n",
       "1272  0.869605  0.928409  0.879734  0.677445  0.655470  0.4375  \n",
       "1141  0.654335  0.832240  0.826804  0.652120  0.517241  0.6875  \n",
       "1166  0.672420  0.828877  0.890187  0.770591  0.576019  0.3750  \n",
       "1143  0.694831  0.871694  0.846881  0.601470  0.361177  0.5000  \n",
       "1301  0.601619  0.787812  0.776307  0.713005  0.396552  0.5625  \n",
       "1163  0.912730  0.997578  0.827473  0.544699  1.000000  0.3125  \n",
       "1140  0.671936  0.873126  0.710287  0.636700  0.325362  0.7500  \n",
       "1181  0.850920  0.825952  0.732199  0.780328  0.663793  0.1875  \n",
       "1161  0.667550  0.839062  0.820416  0.489617  0.547414  0.8750  \n",
       "1211  0.737734  0.905421  0.833222  0.624855  0.454672  0.5625  \n",
       "1153  0.750552  0.836423  0.650284  0.475410  0.549569  0.5000  \n",
       "1228  0.601619  0.908189  0.966856  0.642623  0.400862  0.2500  \n",
       "1443  0.751691  0.831603  0.751265  0.648546  0.458843  0.8125  \n",
       "1242  0.883885  0.961822  0.885696  0.930273  0.633621  0.1250  \n",
       "1429  0.780868  0.799731  0.735728  0.465137  0.258621  0.9375  \n",
       "1266  0.810155  0.860685  0.863894  0.550820  0.471059  0.1875  \n",
       "1221  0.745985  0.869228  0.815853  0.731261  0.579667  0.8750  \n",
       "1281  0.833440  0.926182  0.727361  0.524167  0.558954  0.3750  \n",
       "1356  0.669610  0.803306  0.718084  0.731366  0.413793  0.6875  \n",
       "1323  0.703838  0.960784  0.894689  0.684082  0.729978  0.3125  \n",
       "1454  0.712877  0.822376  0.905104  0.807869  0.465517  0.7500  \n",
       "1328  0.749963  0.892695  0.749842  0.636503  0.487069  0.0625  \n",
       "1354  0.801472  0.867666  0.543415  0.734426  0.375000  1.0000  \n",
       "1390  0.849391  0.898501  0.775169  0.506399  0.433815  0.2500  \n",
       "1360  0.713024  0.942412  0.867675  0.495082  0.443350  0.8125  \n",
       "...        ...       ...       ...       ...       ...     ...  \n",
       "1242  0.662746  0.906257  0.751723  0.679756  0.687500  0.0625  \n",
       "1268  0.579367  0.884888  0.684013  0.578402  0.923828  0.3125  \n",
       "1328  0.673637  0.950435  0.706813  0.694712  0.902344  0.1250  \n",
       "1401  0.805915  0.881592  0.821173  0.695266  0.647059  0.1875  \n",
       "1332  0.738923  0.811940  0.663285  0.765107  1.000000  0.0625  \n",
       "1181  0.769394  0.815016  0.655915  0.643177  0.718750  0.2500  \n",
       "1437  0.582255  0.891542  0.785408  0.677515  0.586397  0.1250  \n",
       "1274  0.608826  0.847886  0.623212  0.609837  0.623047  0.1875  \n",
       "1314  0.881701  0.911443  0.881259  0.692308  0.753217  0.0625  \n",
       "1231  0.756123  0.857400  0.776735  0.697855  0.682129  0.3125  \n",
       "1323  0.685421  0.851057  0.655132  0.562685  0.671387  0.3750  \n",
       "1458  0.718808  0.793968  0.563931  0.590976  0.547852  0.4375  \n",
       "1393  0.742375  0.802425  0.684013  0.814164  0.698242  0.6250  \n",
       "1211  0.657925  0.977923  0.674893  0.502959  0.515625  0.6875  \n",
       "1438  0.563715  0.775034  0.697186  0.551730  0.505208  0.0625  \n",
       "1235  0.571511  0.912376  0.804095  0.619268  0.725098  0.2500  \n",
       "1328  0.673637  0.950435  0.706813  0.694712  0.902344  0.1250  \n",
       "1332  0.738923  0.811940  0.663285  0.765107  1.000000  0.0625  \n",
       "1437  0.582255  0.891542  0.785408  0.677515  0.586397  0.1250  \n",
       "1242  0.662746  0.906257  0.751723  0.679756  0.687500  0.0625  \n",
       "1314  0.881701  0.911443  0.881259  0.692308  0.753217  0.0625  \n",
       "1323  0.685421  0.851057  0.655132  0.562685  0.671387  0.3750  \n",
       "1393  0.742375  0.802425  0.684013  0.814164  0.698242  0.6250  \n",
       "1438  0.563715  0.775034  0.697186  0.551730  0.505208  0.0625  \n",
       "1314  0.881701  0.911443  0.881259  0.692308  0.753217  0.0625  \n",
       "1393  0.742375  0.802425  0.684013  0.814164  0.698242  0.6250  \n",
       "1437  0.582255  0.891542  0.785408  0.677515  0.586397  0.1250  \n",
       "1328  0.673637  0.950435  0.706813  0.694712  0.902344  0.1250  \n",
       "1437  0.582255  0.891542  0.785408  0.677515  0.586397  0.1250  \n",
       "1314  0.881701  0.911443  0.881259  0.692308  0.753217  0.0625  \n",
       "\n",
       "[1426 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_team_ones, new_team_twos, one_minus_two = gen_new_data(team_vector_store)\n",
    "\n",
    "# display(new_team_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function to create the third model. This model has 3 hidden layers, and 15 units (1 for each feature in the feature vector) on each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reg_model(team_feature_length):\n",
    "    \n",
    "    t1 = Input(shape = (team_feature_length, ))\n",
    "    t2 = Input(shape = (team_feature_length, ))\n",
    "    \n",
    "    team_transform = Dense(team_feature_length, \n",
    "                            activation = 'tanh', \n",
    "                            kernel_regularizer = l1_l2(0, 0.001)\n",
    "                          )\n",
    "    \n",
    "    team_1 = team_transform(t1)\n",
    "    team_2 = team_transform(t2)\n",
    "    \n",
    "    merge = Concatenate(axis=-1)([team_1, team_2])\n",
    "    \n",
    "    dropout = Dropout(0.2)(merge)\n",
    "    \n",
    "    x = Dense(team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(dropout)\n",
    "    \n",
    "    x = Dense(team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(x)\n",
    "    \n",
    "    x = Dense(team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(x)\n",
    "    \n",
    "    pred = Dense(1)(x) #linear activation for regression\n",
    "    model = Model(inputs = [t1, t2],\n",
    "                 outputs = pred)\n",
    "    model.compile(optimizer = 'Adam', loss = 'mae', metrics = [pred_percentage])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def new_train_and_evaluate(model, X_train, Y_train, X_test, Y_test, epochs):\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = 1)\n",
    "    loss = model.evaluate(X_test, Y_test, batch_size = 1)\n",
    "    print(\"Loss was: {} and prediction percentage was: {}\".format(loss[0], loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we test this third neural network using the same process as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold: 1\n",
      "Epoch 1/5\n",
      "1267/1267 [==============================] - 4s 3ms/step - loss: 10.7253 - pred_percentage: 0.6361\n",
      "Epoch 2/5\n",
      "1267/1267 [==============================] - 3s 2ms/step - loss: 9.8333 - pred_percentage: 0.6906A: 1s -\n",
      "Epoch 3/5\n",
      "1267/1267 [==============================] - 3s 2ms/step - loss: 9.5586 - pred_percentage: 0.7088\n",
      "Epoch 4/5\n",
      "1267/1267 [==============================] - 4s 3ms/step - loss: 9.5204 - pred_percentage: 0.7080\n",
      "Epoch 5/5\n",
      "1267/1267 [==============================] - 4s 3ms/step - loss: 9.4946 - pred_percentage: 0.6993\n",
      "159/159 [==============================] - 0s 3ms/step\n",
      "Loss was: 8.517918432002547 and accuracy was: 0.7484276729559748\n",
      "Running fold: 2\n",
      "Epoch 1/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 10.7481 - pred_percentage: 0.6421\n",
      "Epoch 2/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 9.7585 - pred_percentage: 0.6994\n",
      "Epoch 3/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 9.5919 - pred_percentage: 0.7041A: 1s - \n",
      "Epoch 4/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 9.5449 - pred_percentage: 0.7025A: 0s - loss: 9.6215 - pred\n",
      "Epoch 5/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 9.3750 - pred_percentage: 0.7080\n",
      "152/152 [==============================] - 0s 2ms/step\n",
      "Loss was: 9.585701451293732 and accuracy was: 0.6710526315789473\n",
      "Running fold: 3\n",
      "Epoch 1/5\n",
      "1272/1272 [==============================] - 4s 3ms/step - loss: 10.7308 - pred_percentage: 0.6447\n",
      "Epoch 2/5\n",
      "1272/1272 [==============================] - 3s 2ms/step - loss: 9.6404 - pred_percentage: 0.7075\n",
      "Epoch 3/5\n",
      "1272/1272 [==============================] - 3s 3ms/step - loss: 9.4336 - pred_percentage: 0.7060\n",
      "Epoch 4/5\n",
      "1272/1272 [==============================] - 3s 3ms/step - loss: 9.4165 - pred_percentage: 0.7099\n",
      "Epoch 5/5\n",
      "1272/1272 [==============================] - 3s 2ms/step - loss: 9.3481 - pred_percentage: 0.7107\n",
      "154/154 [==============================] - 0s 2ms/step\n",
      "Loss was: 9.699446547921601 and accuracy was: 0.6558441558441559\n",
      "Running fold: 4\n",
      "Epoch 1/5\n",
      "1276/1276 [==============================] - 4s 3ms/step - loss: 10.5490 - pred_percentage: 0.6411: 1s -\n",
      "Epoch 2/5\n",
      "1276/1276 [==============================] - 4s 3ms/step - loss: 9.6903 - pred_percentage: 0.7006A - ETA: 0s - loss: 9.771\n",
      "Epoch 3/5\n",
      "1276/1276 [==============================] - 3s 2ms/step - loss: 9.6135 - pred_percentage: 0.7006\n",
      "Epoch 4/5\n",
      "1276/1276 [==============================] - 3s 3ms/step - loss: 9.5024 - pred_percentage: 0.6897\n",
      "Epoch 5/5\n",
      "1276/1276 [==============================] - 3s 3ms/step - loss: 9.5165 - pred_percentage: 0.7069A: 1s - loss: 9\n",
      "150/150 [==============================] - 0s 3ms/step\n",
      "Loss was: 9.267846923073133 and accuracy was: 0.76\n",
      "Running fold: 5\n",
      "Epoch 1/5\n",
      "1278/1278 [==============================] - 4s 3ms/step - loss: 10.6534 - pred_percentage: 0.6573\n",
      "Epoch 2/5\n",
      "1278/1278 [==============================] - 3s 2ms/step - loss: 9.7624 - pred_percentage: 0.6941\n",
      "Epoch 3/5\n",
      "1278/1278 [==============================] - 3s 3ms/step - loss: 9.6495 - pred_percentage: 0.7003\n",
      "Epoch 4/5\n",
      "1278/1278 [==============================] - 3s 2ms/step - loss: 9.4523 - pred_percentage: 0.7113\n",
      "Epoch 5/5\n",
      "1278/1278 [==============================] - 3s 3ms/step - loss: 9.3744 - pred_percentage: 0.7081\n",
      "148/148 [==============================] - 0s 3ms/step\n",
      "Loss was: 9.458887942689094 and accuracy was: 0.6891891891891891\n",
      "Running fold: 6\n",
      "Epoch 1/5\n",
      "1283/1283 [==============================] - 4s 3ms/step - loss: 10.6010 - pred_percentage: 0.6345\n",
      "Epoch 2/5\n",
      "1283/1283 [==============================] - 3s 2ms/step - loss: 9.6126 - pred_percentage: 0.6976\n",
      "Epoch 3/5\n",
      "1283/1283 [==============================] - 3s 2ms/step - loss: 9.5181 - pred_percentage: 0.6937A: 1s - loss: 9.4639 - \n",
      "Epoch 4/5\n",
      "1283/1283 [==============================] - 3s 2ms/step - loss: 9.2991 - pred_percentage: 0.7124A: 1s - loss: 9.6035 - p - ETA: 0s - loss: 9.1988 - pred_percenta\n",
      "Epoch 5/5\n",
      "  74/1283 [>.............................] - ETA: 2s - loss: 9.5017 - pred_percentage: 0.6757   "
     ]
    }
   ],
   "source": [
    "# display(one_minus_two)\n",
    "team_feature_length = len(imp_attributes)\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(one_minus_two, n_folds = n_folds, shuffle = True)\n",
    "# folds = StratifiedKFold(n_folds = n_folds, shuffle=True)\n",
    "\n",
    "percentages = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = make_reg_model(team_feature_length)\n",
    "    percentage = train_and_evaluate(model,\n",
    "                       [new_team_ones.iloc[train], new_team_twos.iloc[train]],\n",
    "                       one_minus_two.iloc[train], \n",
    "                       [new_team_ones.iloc[test], new_team_twos.iloc[test]],\n",
    "                       one_minus_two.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    percentages.append(percentage)\n",
    "\n",
    "print(\"Average prediction accuracy: {}\".format(np.mean(percentages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created and tested 3 different models:\n",
    "1. Simple binary classifier. \n",
    "    - Uses our definition of matchups (team_1 - team_2), and outputs 1 if team_1 wins, 0 if team_2 wins.\n",
    "\n",
    "2. Simple Regression.\n",
    "    - Uses our definition of matchups, and tries to predict the difference between points of team_1 and team_2.\n",
    "    - We then use a scoring function (return 1 if score_diff > 0, and 0 otherwise) in order to generate predictions.\n",
    "\n",
    "3. More Complex Regression.\n",
    "    - We feed just the team vectors into the model, and let the model learn an effective definition for what a matchup is.\n",
    "    - The model then outputs a prediction of the score differential (team_1_score - team_2_score).\n",
    "    - We then use the scoring function defined above to generate predictions.\n",
    "    \n",
    "    \n",
    "\n",
    "Now we will use these models (and the training data) in order to generate predictions for the matchups between 2011-2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matchup_testing_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Function to generate the training/testing data.\n",
    "    Based on team_vector_store and the tourney detailed results.\n",
    "    If classification = True, generates for binary classification.\n",
    "    If classification = False, generates for regression.\n",
    "    \n",
    "    Returns training_data\n",
    "    \"\"\"\n",
    "    col_names = list(team_vector_store[2003])\n",
    "\n",
    "    testing_matchup_data = pd.DataFrame(columns = col_names)\n",
    "    matchups = []\n",
    "    \n",
    "    for year in [2011, 2012, 2013]:\n",
    "        season_data = team_vector_store[year].sort_index()\n",
    "        for i1, row1 in season_data.iterrows():\n",
    "            for i2, row2 in season_data.iterrows():\n",
    "#                 print(row2)\n",
    "                if i2 > i1:\n",
    "                    matchups.append([year, i1, i2])\n",
    "                    matchup = row1 - row2\n",
    "                    testing_matchup_data = testing_matchup_data.append([matchup], ignore_index = True)\n",
    "\n",
    "    return testing_matchup_data, matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_data, matchups = generate_matchup_testing_data(team_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_to_csv(model, X_train, Y_train, outfile, testing_data, matchups, classification = True):\n",
    "    model.fit(X_train, Y_train, epochs = 5, batch_size = 1)\n",
    "    if classification:\n",
    "        predictions = model.predict_classes(testing_data)\n",
    "    else:\n",
    "        predictions = model.predict(testing_data)\n",
    "        \n",
    "    result = []\n",
    "    for i in range(len(matchups)):\n",
    "        representation = '_'.join([str(matchups[i][0]), str(matchups[i][1]), str(matchups[i][2])])\n",
    "        if classification:\n",
    "            representation = [representation] + [str(predictions[i][0])]\n",
    "        else:\n",
    "            pred = 1 if predictions[i][0] > 0 else 0\n",
    "            representation = [representation] + [str(pred)]\n",
    "        result.append(representation)\n",
    "\n",
    "    # print(result)\n",
    "    result = pd.DataFrame(data = result, columns = ['game_ID', 'Prediction'])\n",
    "    result.to_csv(outfile, index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 15)                210       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 706\n",
      "Trainable params: 706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1426/1426 [==============================] - 5s 4ms/step - loss: 0.6260 - acc: 0.6935\n",
      "Epoch 2/5\n",
      "1426/1426 [==============================] - 5s 3ms/step - loss: 0.5899 - acc: 0.7111\n",
      "Epoch 3/5\n",
      "1426/1426 [==============================] - 4s 3ms/step - loss: 0.5818 - acc: 0.7153\n",
      "Epoch 4/5\n",
      "1426/1426 [==============================] - 4s 3ms/step - loss: 0.5779 - acc: 0.7139\n",
      "Epoch 5/5\n",
      "1426/1426 [==============================] - 4s 3ms/step - loss: 0.5747 - acc: 0.7132\n"
     ]
    }
   ],
   "source": [
    "X_train_class = train_matchup_data_class.drop(['class'], axis = 1)\n",
    "input_shape = X_train_class.shape\n",
    "Y_train_class = train_matchup_data_class['class']\n",
    "binary_class_model = build_keras_model(input_shape, \n",
    "                                       num_units = 15, \n",
    "                                       num_layers = 3, \n",
    "                                       classification = True)\n",
    "\n",
    "# plot_model(binary_class_model, to_file = \"binary_class_model.png\")\n",
    "print(binary_class_model.summary())\n",
    "\n",
    "generate_predictions_to_csv(binary_class_model, \n",
    "                            X_train_class, \n",
    "                            Y_train_class, \n",
    "                            \"bin_predictions.csv\",\n",
    "                            matchup_data,\n",
    "                            matchups,\n",
    "                            classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 15)                210       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 706\n",
      "Trainable params: 706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 10.1738 - pred_percentage: 0.6788\n",
      "Epoch 2/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.2273 - pred_percentage: 0.7132\n",
      "Epoch 3/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.1752 - pred_percentage: 0.7104\n",
      "Epoch 4/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.1287 - pred_percentage: 0.7090\n",
      "Epoch 5/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.0347 - pred_percentage: 0.7195A: 0s - loss: 8.9981 - pred_percentage: 0. - ETA: 0s - loss: 9.0241 - pred_percentag\n"
     ]
    }
   ],
   "source": [
    "X_train_reg = train_matchup_data_reg.drop(['class'], axis = 1)\n",
    "input_shape = X_train_reg.shape\n",
    "Y_train_reg = train_matchup_data_reg['class']\n",
    "\n",
    "simple_regression_model = build_keras_model(input_shape, \n",
    "                                            num_units = 15, \n",
    "                                            num_layers = 3, \n",
    "                                            classification = False)\n",
    "print(simple_regression_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "generate_predictions_to_csv(simple_regression_model, \n",
    "                            X_train_reg, \n",
    "                            Y_train_reg, \n",
    "                            \"simple_reg_predictions.csv\",\n",
    "                            matchup_data,\n",
    "                            matchups,\n",
    "                            classification = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indiv_testing_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Function to generate the testing data.\n",
    "    \"\"\"\n",
    "    col_names = list(team_vector_store[2003])\n",
    "\n",
    "    team_ones = pd.DataFrame(columns = col_names)\n",
    "    team_twos = pd.DataFrame(columns = col_names)\n",
    "    matchups = []\n",
    "    \n",
    "    for year in [2011, 2012, 2013]:\n",
    "        season_data = team_vector_store[year].sort_index()\n",
    "        for i1, row1 in season_data.iterrows():\n",
    "            for i2, row2 in season_data.iterrows():\n",
    "#                 print(row2)\n",
    "                if i2 > i1:\n",
    "                    team_ones = team_ones.append(row1, ignore_index = True)\n",
    "                    team_twos = team_twos.append(row2, ignore_index = True)\n",
    "                    matchups.append([year, i1, i2])\n",
    "                    \n",
    "    return [team_ones, team_twos], matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [new_team_ones, new_team_twos]\n",
    "team_dim = len(imp_attributes)\n",
    "Y_train = one_minus_two\n",
    "\n",
    "complex_regression_model = make_reg_model(team_dim)\n",
    "indiv_testing_data, matchups = generate_indiv_testing_data(team_vector_store)\n",
    "\n",
    "print(complex_regression_model.summary())\n",
    "\n",
    "\n",
    "generate_predictions_to_csv(complex_regression_model,\n",
    "                           X_train,\n",
    "                           Y_train,\n",
    "                           \"complex_reg_predictions.csv\",\n",
    "                           indiv_testing_data,\n",
    "                           matchups,\n",
    "                           classification = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
