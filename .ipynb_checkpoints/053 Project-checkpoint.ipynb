{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import csv\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we clean up the data. We create 2 dataframes, one for winners and one for losers. Each has the same column names (team, score, fgm, fga, fgm3, fga3, ftm, fta, or, dr, ast, stl, blk, pf).\n",
    "<br> We will now use these dataframes to build our representative team vectors for the season, putting together the information from the winners and the losers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = ['team', 'score', 'fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'stl', 'blk', 'pf']\n",
    "imp_attributes = ['team', 'score', 'fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'stl', 'blk']\n",
    "\n",
    "def load_team_data():\n",
    "    reg_season_data_filename = \"RegularSeasonDetailedResults.csv\"\n",
    "    #We load the data into a pandas dataframe.\n",
    "    reg_season_data = pd.read_csv(reg_season_data_filename)\n",
    "    # reg_season_data = reg_season_data.loc[lambda df: df.Season == 2003]\n",
    "    #Took out location and overtimes. Play around with what other stats we want to include. \n",
    "    \n",
    "    w = reg_season_data[['Season'] + ['W' + x for x in imp_attributes]]\n",
    "    win_remap = {'W'+x: x for x in imp_attributes}\n",
    "    winners = w.rename(index = str, columns = win_remap)\n",
    "    los_remap = {'L' + x:x for x in imp_attributes}\n",
    "    l = reg_season_data[['Season']+ ['L' + x for x in imp_attributes]]\n",
    "    losers = l.rename(index=str, columns = los_remap)\n",
    "    \n",
    "    #Know what seasons of data we are dealing with\n",
    "    reg_season_years = reg_season_data['Season'].unique()\n",
    "\n",
    "    #Here, we put all the data (from both winners and losers) into one dataframe. \n",
    "    team_data = pd.concat([winners, losers])\n",
    "\n",
    "    by_reg_season = {}\n",
    "    for year in reg_season_years:\n",
    "        s = team_data[team_data['Season']==year]\n",
    "    #     display(s)\n",
    "        s = s.drop(['Season'], axis = 1)\n",
    "        by_reg_season[year] = s\n",
    "    return by_reg_season, reg_season_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "by_reg_season, reg_season_years = load_team_data()\n",
    "# display(by_reg_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denominate the team vector to be represented as:\n",
    "\\[Score, FG Made, FG Attempted, 3 Pointers Made, 3 Pointers attempted, FT made, FT Attempted, Off Rebounds, Def Rebounds, Assists, Steals, Blocks\\]. (Later in the code we add the team's seed to their vectors)\n",
    "<br> <br>\n",
    "In this following cell we aggregate the data and calculate each team's average statistics for each year. To store this, we keep 1 dataframe (indexed by team) for each year of data. The pointers to these objects are stored in our dictionary, which serves as a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_vector_store(by_reg_season, reg_season_years):\n",
    "    # Create dictionary mapping year-> Dataframe of teams within that year.\n",
    "    team_vector_store = {}\n",
    "\n",
    "    for year in reg_season_years:\n",
    "        t = by_reg_season[year].groupby('team')[imp_attributes[1:]].mean()\n",
    "        team_vector_store[year] = t.apply(lambda x: x, axis = 0)\n",
    "    \n",
    "    return team_vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_vector_store = get_team_vector_store(by_reg_season, reg_season_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to normalize the data. This could be used at whatever point we want. If we want a standardn normal distribution, set standard = True, otherwise we let standard=False for min/max normalization.\n",
    "\n",
    "We test using both methods of normalization to see which gave us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(team_vector_store, standard = False):\n",
    "    \"\"\"\n",
    "    Function to normalize the data. \n",
    "    IMPORTANT: Modifies the argument passed in.\n",
    "    Set standard to True if normalizing to standard Gaussian,\n",
    "    False for min/max normalization. \n",
    "    \"\"\"\n",
    "    for y in reg_season_years:\n",
    "        if standard:\n",
    "            team_vector_store[y] = team_vector_store[y].apply(lambda x: (x - x.mean())/x.std(), axis = 0)\n",
    "        else:\n",
    "            team_vector_store[y] = team_vector_store[y].apply(lambda x: x/x.max(), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a matchup to be the difference between the vectors of team1 and team2. If team 1 wins, we classify the matchup as a 1, while we classify it as a 0 if team2 wins. Below is a quick example of how it could work. However, we must now add each team's seed to their representative vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "m = team_vector_store[year]\n",
    "# display(m.loc[1274])\n",
    "# display(m.loc[1199])\n",
    "matchup_1 = (m.loc[1274] - m.loc[1199])\n",
    "matchup_1['class'] = 1\n",
    "# display(matchup_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we modify each season's team vector dataframe by adding each team's NCAA tournament seed. This will allow us to use this as a feature when outputting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed_data(filename):\n",
    "    tourney_seeds_data = pd.read_csv(filename)\n",
    "    tourney_seeds_data = tourney_seeds_data.query('Season >= 2003')\n",
    "    tourney_seeds_data = tourney_seeds_data.reset_index(drop=True)\n",
    "    tourney_seeds_data.rename(str.lower, axis='columns', inplace = True)\n",
    "    tourney_seeds_data['seed'] = tourney_seeds_data['seed'].str.extract('(\\d+)', expand = False).astype(int)\n",
    "    return tourney_seeds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_seeds_data = load_seed_data(\"TourneySeeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vector_store(team_vector_store, tourney_seeds_data):\n",
    "    #Joins the team vector store with the seed information.\n",
    "    #Allows us to consider seed as an attribute of the team.\n",
    "    #Eliminates the teams not in the tournament.\n",
    "    for s in reg_season_years:\n",
    "        relevant_data = tourney_seeds_data.query('season == {}'.format(s))\n",
    "        relevant_data = relevant_data.drop('season', axis = 1)\n",
    "        m = team_vector_store[s]\n",
    "        m = m.merge(relevant_data, how = 'outer', left_index = True, right_on = 'team')\n",
    "        m.set_index('team', inplace=True)\n",
    "        m = m[pd.notnull(m['seed'])]\n",
    "        team_vector_store[s] = m\n",
    "    return team_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_vector_store = update_vector_store(team_vector_store, tourney_seeds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(team_vector_store, standard = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must use our team vectors along with the matchups each season to build our feature vectors, which we will use for classification. A 1 will signify that the 1st team won, while a 0 means the 2nd team won.\n",
    "\n",
    "Our NCAA tournament data is in the same format as the regular season one. In order to have a combination of 1s and 0s, we will generate 2 different vectors for each matchup, where the first has team_1 = winning_team while the second has team_1 = losing_team. Because of the way our data is labeled, if we do not do this then all of our vectors will have classification 1 or 0, which won't allow our model to actually learn a decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(team_vector_store, classification = True):\n",
    "    \"\"\"\n",
    "    Function to generate the training/testing data.\n",
    "    Based on team_vector_store and the tourney detailed results.\n",
    "    If classification = True, generates for binary classification.\n",
    "    If classification = False, generates for regression.\n",
    "    \n",
    "    Returns training_data\n",
    "    \"\"\"\n",
    "    tourney_game_data = pd.read_csv(\"TourneyDetailedResults.csv\")\n",
    "    tourney_game_data = tourney_game_data[['Season', 'Wteam', 'Lteam', 'Wscore', 'Lscore']]\n",
    "\n",
    "\n",
    "    col_names = list(team_vector_store[2003]) + ['class']\n",
    "\n",
    "    train_matchup_data = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    for index, row in tourney_game_data.iterrows():\n",
    "        season, wteam, lteam, wscore, lscore = row\n",
    "        \n",
    "        #Winner is team 1, loser is team 2\n",
    "        game_vector_1 = team_vector_store[season].loc[wteam] - team_vector_store[season].loc[lteam]\n",
    "        if classification:\n",
    "            game_vector_1['class'] = 1\n",
    "        else:\n",
    "            game_vector_1['class'] = wscore - lscore\n",
    "        \n",
    "        #Loser is team 1, winner is team 2\n",
    "        game_vector_2 = team_vector_store[season].loc[lteam] - team_vector_store[season].loc[wteam]\n",
    "        if classification:\n",
    "            game_vector_2['class'] = 0\n",
    "        else:\n",
    "            game_vector_2['class'] = lscore - wscore\n",
    "        \n",
    "        train_matchup_data = train_matchup_data.append([game_vector_1, game_vector_2], ignore_index = True)\n",
    "        \n",
    "    return train_matchup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matchup_data_class = generate_training_data(team_vector_store, classification = True)\n",
    "train_matchup_data_reg = generate_training_data(team_vector_store, classification = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our training data built up. The X data will just be the rows except ('class'), while the Y data will simply be the column 'class'.\n",
    "\n",
    "Below, we define functions to build 2 different kinds of simple neural networks:\n",
    "1. A binary classification network: \n",
    "    - Takes matchup vectors (team_1 - team_2) as inputs, and outputs a binary digit prediction (1 if team_1 wins, 0 if team_2 wins).\n",
    "2. A regression network: \n",
    "    - Takes matchup vectors (team_1 - team_2) as inputs, and outputs a scalar prediction for the difference between scores (team_1_score - team_2_score).\n",
    "    - We then use these predicted scores to predict the matchup victor (1 if predicted score > 0, and 0 otherwise)\n",
    "    \n",
    "In order to evaluate the regression network's predictions, we define a prediction_percentage function which returns the percentage of games in which the victor is predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_percentage(y_true, y_pred):\n",
    "    y_true_sign = tf.sign(y_true)\n",
    "    y_pred_sign = tf.sign(y_pred)\n",
    "    mults = tf.multiply(y_true_sign, y_pred_sign)\n",
    "    return K.mean(mults+1)/2\n",
    "\n",
    "def build_keras_model(input_shape, num_units, num_layers, classification = True):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(units = num_units, \n",
    "                    input_dim = input_shape[1], \n",
    "                    activation = 'relu',\n",
    "                    kernel_regularizer = l1_l2(0, 0.001)))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dense(num_units, \n",
    "                        activation = 'relu',\n",
    "                        kernel_regularizer = l1_l2(0, 0.001)))\n",
    "#         model.add(Dropout(0.25))\n",
    "    \n",
    "    if classification:\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = Adam(), metrics = ['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss = 'mae', optimizer = Adam(), metrics = [pred_percentage])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, X_train, Y_train, X_test, Y_test, epochs):\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = 1)\n",
    "    loss = model.evaluate(X_test, Y_test, batch_size = 1)\n",
    "    print(\"Loss was: {} and accuracy was: {}\".format(loss[0], loss[1]))\n",
    "    return loss[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use cross-validation (with 10 folds) to test the performance of our binary classification neural network. This process splits the data into 10 randomized chunks. Then, it uses $\\frac{9}{10}$ chunks to train the network, while using the final chunk to test the predictions. After this, the average accuracy is printed below, and serves as a proxy for the performance of this neural network on unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold: 1\n",
      "Epoch 1/5\n",
      "1282/1282 [==============================] - 3s 2ms/step - loss: 0.6394 - acc: 0.6755\n",
      "Epoch 2/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5953 - acc: 0.7246\n",
      "Epoch 3/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5828 - acc: 0.7176\n",
      "Epoch 4/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5750 - acc: 0.7192\n",
      "Epoch 5/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5703 - acc: 0.7270\n",
      "144/144 [==============================] - 0s 1ms/step\n",
      "Loss was: 0.5936589991777308 and accuracy was: 0.6458333333333334\n",
      "Running fold: 2\n",
      "Epoch 1/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.6329 - acc: 0.6942\n",
      "Epoch 2/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5965 - acc: 0.7059\n",
      "Epoch 3/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5827 - acc: 0.7161\n",
      "Epoch 4/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5778 - acc: 0.7168\n",
      "Epoch 5/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5717 - acc: 0.7262\n",
      "144/144 [==============================] - 0s 1ms/step\n",
      "Loss was: 0.5678695892501209 and accuracy was: 0.7083333333333334\n",
      "Running fold: 3\n",
      "Epoch 1/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.6433 - acc: 0.6833\n",
      "Epoch 2/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5907 - acc: 0.7083\n",
      "Epoch 3/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5792 - acc: 0.7161\n",
      "Epoch 4/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5757 - acc: 0.7145\n",
      "Epoch 5/5\n",
      "1282/1282 [==============================] - 2s 2ms/step - loss: 0.5689 - acc: 0.7161\n",
      "144/144 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.5718523275831507 and accuracy was: 0.7291666666666666\n",
      "Running fold: 4\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6315 - acc: 0.7025\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5895 - acc: 0.7227\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5749 - acc: 0.7235\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5695 - acc: 0.7165\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5654 - acc: 0.7220\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.6356644554352257 and accuracy was: 0.6549295774647887\n",
      "Running fold: 5\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6377 - acc: 0.6885\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5924 - acc: 0.7103A: \n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5785 - acc: 0.7196\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5797 - acc: 0.7227\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5712 - acc: 0.7157\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.5737729076243622 and accuracy was: 0.7323943661971831\n",
      "Running fold: 6\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6310 - acc: 0.6893\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5917 - acc: 0.7134\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5844 - acc: 0.7056\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5824 - acc: 0.7165\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5710 - acc: 0.7134\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.5473388294413896 and accuracy was: 0.7676056338028169\n",
      "Running fold: 7\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6312 - acc: 0.6900\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5787 - acc: 0.7204\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5743 - acc: 0.7181\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5752 - acc: 0.7157\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5737 - acc: 0.7150\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.6110743690596919 and accuracy was: 0.7112676056338029\n",
      "Running fold: 8\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6368 - acc: 0.6783\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.6036 - acc: 0.7033\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5863 - acc: 0.7072\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5762 - acc: 0.7188A: 0s - loss: 0.5827 - acc: 0.\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5775 - acc: 0.7103\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.5394603066461187 and accuracy was: 0.7394366197183099\n",
      "Running fold: 9\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 0.6400 - acc: 0.6900A: 2s \n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.6002 - acc: 0.7079\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5907 - acc: 0.7064\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5812 - acc: 0.7188\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5723 - acc: 0.7103\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.5232715436165601 and accuracy was: 0.7253521126760564\n",
      "Running fold: 10\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 4s 3ms/step - loss: 0.6311 - acc: 0.6877\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5959 - acc: 0.7025\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5725 - acc: 0.7173\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5731 - acc: 0.7173\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 0.5662 - acc: 0.7235\n",
      "142/142 [==============================] - 0s 2ms/step\n",
      "Loss was: 0.6112252302257948 and accuracy was: 0.6830985915492958\n",
      "The average accuracy of predictions was: 0.7097417840375587\n"
     ]
    }
   ],
   "source": [
    "X_train_class = train_matchup_data_class.drop(['class'], axis = 1)\n",
    "input_shape = X_train_class.shape\n",
    "Y_train_class = train_matchup_data_class['class']\n",
    "\n",
    "num_units = 2*len(imp_attributes)\n",
    "num_layers = 3\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(Y_train_class, n_folds = n_folds, shuffle = True)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = build_keras_model(input_shape, num_units, num_layers, classification = True)\n",
    "    accuracy = train_and_evaluate(model, X_train_class.iloc[train], \n",
    "                       Y_train_class.iloc[train], \n",
    "                       X_train_class.iloc[test], \n",
    "                       Y_train_class.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"The average accuracy of predictions was: {}\".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the performance of our regression neural network. As above, we use cross validation (with 10 folds) in order to test our network on our given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1262/1262 [==============================] - 3s 2ms/step - loss: 9.7974 - pred_percentage: 0.6751\n",
      "Epoch 2/5\n",
      "1262/1262 [==============================] - 2s 2ms/step - loss: 9.1343 - pred_percentage: 0.7171\n",
      "Epoch 3/5\n",
      "1262/1262 [==============================] - 2s 2ms/step - loss: 9.0361 - pred_percentage: 0.7084\n",
      "Epoch 4/5\n",
      "1262/1262 [==============================] - 2s 2ms/step - loss: 8.9753 - pred_percentage: 0.7163\n",
      "Epoch 5/5\n",
      "1262/1262 [==============================] - 2s 2ms/step - loss: 8.9432 - pred_percentage: 0.7139\n",
      "164/164 [==============================] - 0s 2ms/step\n",
      "Loss was: 9.243078572357573 and accuracy was: 0.6951219512195121\n",
      "Running fold: 2\n",
      "Epoch 1/5\n",
      "1269/1269 [==============================] - 3s 2ms/step - loss: 9.8959 - pred_percentage: 0.6887\n",
      "Epoch 2/5\n",
      "1269/1269 [==============================] - 2s 2ms/step - loss: 9.1173 - pred_percentage: 0.7147\n",
      "Epoch 3/5\n",
      "1269/1269 [==============================] - 2s 2ms/step - loss: 9.0734 - pred_percentage: 0.7179\n",
      "Epoch 4/5\n",
      "1269/1269 [==============================] - 2s 2ms/step - loss: 9.0010 - pred_percentage: 0.7250\n",
      "Epoch 5/5\n",
      "1269/1269 [==============================] - 2s 2ms/step - loss: 8.9180 - pred_percentage: 0.7195\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "Loss was: 10.020169482299476 and accuracy was: 0.6369426751592356\n",
      "Running fold: 3\n",
      "Epoch 1/5\n",
      "1274/1274 [==============================] - 3s 2ms/step - loss: 9.9958 - pred_percentage: 0.7041\n",
      "Epoch 2/5\n",
      "1274/1274 [==============================] - 2s 2ms/step - loss: 9.2577 - pred_percentage: 0.7088\n",
      "Epoch 3/5\n",
      "1274/1274 [==============================] - 2s 2ms/step - loss: 9.0955 - pred_percentage: 0.7151\n",
      "Epoch 4/5\n",
      "1274/1274 [==============================] - 2s 2ms/step - loss: 9.1544 - pred_percentage: 0.7198\n",
      "Epoch 5/5\n",
      "1274/1274 [==============================] - 2s 2ms/step - loss: 9.0747 - pred_percentage: 0.7111\n",
      "152/152 [==============================] - 0s 3ms/step\n",
      "Loss was: 8.797356285988108 and accuracy was: 0.7631578947368421\n",
      "Running fold: 4\n",
      "Epoch 1/5\n",
      "1278/1278 [==============================] - 3s 2ms/step - loss: 9.8818 - pred_percentage: 0.6948\n",
      "Epoch 2/5\n",
      "1278/1278 [==============================] - 2s 2ms/step - loss: 9.2245 - pred_percentage: 0.7097\n",
      "Epoch 3/5\n",
      "1278/1278 [==============================] - 2s 2ms/step - loss: 9.1811 - pred_percentage: 0.7081\n",
      "Epoch 4/5\n",
      "1278/1278 [==============================] - 2s 2ms/step - loss: 9.1538 - pred_percentage: 0.7152A: 0s - loss: 9.0034 - pred_percenta\n",
      "Epoch 5/5\n",
      "1278/1278 [==============================] - 2s 2ms/step - loss: 9.1441 - pred_percentage: 0.7144\n",
      "148/148 [==============================] - 0s 3ms/step\n",
      "Loss was: 8.431226476523522 and accuracy was: 0.7027027027027027\n",
      "Running fold: 5\n",
      "Epoch 1/5\n",
      "1284/1284 [==============================] - 3s 2ms/step - loss: 10.1941 - pred_percentage: 0.6799\n",
      "Epoch 2/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 9.2684 - pred_percentage: 0.7087\n",
      "Epoch 3/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 9.2117 - pred_percentage: 0.7126\n",
      "Epoch 4/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 9.1837 - pred_percentage: 0.7220\n",
      "Epoch 5/5\n",
      "1284/1284 [==============================] - 2s 2ms/step - loss: 9.1393 - pred_percentage: 0.7134\n",
      "142/142 [==============================] - 0s 3ms/step\n",
      "Loss was: 8.294162594936264 and accuracy was: 0.6971830985915493\n",
      "Running fold: 6\n",
      "Epoch 1/5\n",
      "1291/1291 [==============================] - 3s 2ms/step - loss: 10.1009 - pred_percentage: 0.6871\n",
      "Epoch 2/5\n",
      "1291/1291 [==============================] - 2s 2ms/step - loss: 9.2414 - pred_percentage: 0.7080\n",
      "Epoch 3/5\n",
      "1291/1291 [==============================] - 2s 2ms/step - loss: 9.1506 - pred_percentage: 0.7103\n",
      "Epoch 4/5\n",
      "1291/1291 [==============================] - 2s 2ms/step - loss: 9.1695 - pred_percentage: 0.7119\n",
      "Epoch 5/5\n",
      "1291/1291 [==============================] - 2s 2ms/step - loss: 9.1101 - pred_percentage: 0.7134\n",
      "135/135 [==============================] - 1s 4ms/step\n",
      "Loss was: 8.302843140727944 and accuracy was: 0.7407407407407407\n",
      "Running fold: 7\n",
      "Epoch 1/5\n",
      "1292/1292 [==============================] - 4s 3ms/step - loss: 9.8905 - pred_percentage: 0.6842\n",
      "Epoch 2/5\n",
      "1292/1292 [==============================] - 2s 2ms/step - loss: 9.2168 - pred_percentage: 0.7090\n",
      "Epoch 3/5\n",
      "1292/1292 [==============================] - 2s 2ms/step - loss: 8.9492 - pred_percentage: 0.7299\n",
      "Epoch 4/5\n",
      "1292/1292 [==============================] - 3s 2ms/step - loss: 8.9015 - pred_percentage: 0.7221\n",
      "Epoch 5/5\n",
      "1292/1292 [==============================] - 3s 3ms/step - loss: 9.0133 - pred_percentage: 0.7214\n",
      "134/134 [==============================] - 0s 4ms/step\n",
      "Loss was: 9.539671704840304 and accuracy was: 0.7014925373134329\n",
      "Running fold: 8\n",
      "Epoch 1/5\n",
      "1297/1297 [==============================] - 4s 3ms/step - loss: 9.8731 - pred_percentage: 0.6916\n",
      "Epoch 2/5\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 9.1718 - pred_percentage: 0.7124\n",
      "Epoch 3/5\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 9.1077 - pred_percentage: 0.7201A: 0s - loss: 9.1420 - pred_perc\n",
      "Epoch 4/5\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 9.0244 - pred_percentage: 0.7186\n",
      "Epoch 5/5\n",
      "1297/1297 [==============================] - 3s 2ms/step - loss: 9.0141 - pred_percentage: 0.7178\n",
      "129/129 [==============================] - 1s 5ms/step\n",
      "Loss was: 9.05455408863319 and accuracy was: 0.6744186046511628\n",
      "Running fold: 9\n",
      "Epoch 1/5\n",
      "1292/1292 [==============================] - 4s 3ms/step - loss: 10.0149 - pred_percentage: 0.6889\n",
      "Epoch 2/5\n",
      "1292/1292 [==============================] - 3s 2ms/step - loss: 9.3025 - pred_percentage: 0.6989\n",
      "Epoch 3/5\n",
      "1292/1292 [==============================] - 3s 3ms/step - loss: 9.0820 - pred_percentage: 0.6981A: 0s - loss: 9.0360 - pred_percentage: 0\n",
      "Epoch 4/5\n",
      "1292/1292 [==============================] - 3s 2ms/step - loss: 9.0078 - pred_percentage: 0.7043A: 1s - loss: 8.8333 - pr\n",
      "Epoch 5/5\n",
      "1292/1292 [==============================] - 3s 2ms/step - loss: 9.0286 - pred_percentage: 0.7098\n",
      "134/134 [==============================] - 1s 5ms/step\n",
      "Loss was: 8.84872529533372 and accuracy was: 0.7910447761194029\n",
      "Running fold: 10\n",
      "Epoch 1/5\n",
      "1295/1295 [==============================] - 4s 3ms/step - loss: 9.7331 - pred_percentage: 0.7158\n",
      "Epoch 2/5\n",
      "1295/1295 [==============================] - 3s 2ms/step - loss: 9.0948 - pred_percentage: 0.7243\n",
      "Epoch 3/5\n",
      "1295/1295 [==============================] - 4s 3ms/step - loss: 9.0602 - pred_percentage: 0.7066\n",
      "Epoch 4/5\n",
      "1295/1295 [==============================] - 4s 3ms/step - loss: 8.9653 - pred_percentage: 0.7251A: 3s - loss: 8.7555 - pred_per - ETA: 2s - loss: 8.9868 - pred_percenta - ETA: 2s \n",
      "Epoch 5/5\n",
      "1295/1295 [==============================] - 4s 3ms/step - loss: 8.9828 - pred_percentage: 0.7127\n",
      "131/131 [==============================] - 1s 7ms/step\n",
      "Loss was: 9.80056112168627 and accuracy was: 0.6793893129770993\n",
      "The average accuracy of predictions was: 0.708219429421168\n"
     ]
    }
   ],
   "source": [
    "X_train_reg = train_matchup_data_reg.drop(['class'], axis = 1)\n",
    "input_shape = X_train_reg.shape\n",
    "Y_train_reg = train_matchup_data_reg['class']\n",
    "\n",
    "num_units = 2*len(imp_attributes)\n",
    "num_layers = 3\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(Y_train_reg, n_folds = n_folds, shuffle = True)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = build_keras_model(input_shape, num_units, num_layers, classification = False)\n",
    "    accuracy = train_and_evaluate(model, X_train_reg.iloc[train], \n",
    "                       Y_train_reg.iloc[train], \n",
    "                       X_train_reg.iloc[test], \n",
    "                       Y_train_reg.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"The average accuracy of predictions was: {}\".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final neural network we will test is a bit more complex. Instead of taking a matchup vector as input, this neural network is simply provided the two team vectors. This allows the neural network to learn its own definition for a \"matchup\", instead of having to use ours. As with model #2, this is a regression network, which means its predictions represent the difference between the scores of the teams. \n",
    "\n",
    "In order to build this, we have to build our training set in a different way: building two different training sets in conjunction (one for team_1s and the other for team_2s), and then the labels in a third pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Generates data sets for inputs into the third neural network.\n",
    "    \"\"\"\n",
    "    tourney_game_data = pd.read_csv(\"TourneyDetailedResults.csv\")\n",
    "    tourney_game_data = tourney_game_data[['Season', 'Wteam', 'Lteam', 'Wscore', 'Lscore']]\n",
    "\n",
    "    # display(tourney_game_data)\n",
    "\n",
    "    col_names = list(team_vector_store[2003])\n",
    "    # print(list(col_names))\n",
    "\n",
    "    team_ones = pd.DataFrame(columns = col_names)\n",
    "    team_twos = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    one_minus_two = []\n",
    "    \n",
    "    for index, row in tourney_game_data.iterrows():\n",
    "        season, wteam, lteam, wscore, lscore = row\n",
    "        \n",
    "        win_vector = team_vector_store[season].loc[wteam]\n",
    "        lose_vector = team_vector_store[season].loc[lteam]\n",
    "        \n",
    "        #Win minus lose\n",
    "        team_ones = team_ones.append(win_vector)\n",
    "        team_twos = team_twos.append(lose_vector)\n",
    "        one_minus_two.append(wscore-lscore)\n",
    "    \n",
    "        #Lose minus win\n",
    "        team_ones = team_ones.append(lose_vector)\n",
    "        team_twos = team_twos.append(win_vector)\n",
    "        one_minus_two.append(lscore-wscore)\n",
    "    \n",
    "    one_minus_two = pd.Series(one_minus_two)\n",
    "    return team_ones, team_twos, one_minus_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_team_ones, new_team_twos, one_minus_two = gen_new_data(team_vector_store)\n",
    "\n",
    "# display(new_team_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function to create the third model. This model has 3 hidden layers, and 15 units (1 for each feature in the feature vector) on each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reg_model(team_feature_length):\n",
    "    \n",
    "    t1 = Input(shape = (team_feature_length, ))\n",
    "    t2 = Input(shape = (team_feature_length, ))\n",
    "    \n",
    "    team_transform = Dense(team_feature_length, \n",
    "                            activation = 'tanh', \n",
    "                            kernel_regularizer = l1_l2(0, 0.001)\n",
    "                          )\n",
    "    \n",
    "    team_1 = team_transform(t1)\n",
    "    team_2 = team_transform(t2)\n",
    "    \n",
    "    merge = Concatenate(axis=-1)([team_1, team_2])\n",
    "    \n",
    "    dropout = Dropout(0.2)(merge)\n",
    "    \n",
    "    x = Dense(2*team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(dropout)\n",
    "    \n",
    "    x = Dense(2*team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(x)\n",
    "    \n",
    "    x = Dense(2*team_feature_length,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = l1_l2(0, 0.001)\n",
    "             )(x)\n",
    "    \n",
    "    pred = Dense(1)(x) #linear activation for regression\n",
    "    model = Model(inputs = [t1, t2],\n",
    "                 outputs = pred)\n",
    "    model.compile(optimizer = 'Adam', loss = 'mae', metrics = [pred_percentage])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def new_train_and_evaluate(model, X_train, Y_train, X_test, Y_test, epochs):\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = 1)\n",
    "    loss = model.evaluate(X_test, Y_test, batch_size = 1)\n",
    "    print(\"Loss was: {} and prediction percentage was: {}\".format(loss[0], loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we test this third neural network using the same process as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold: 1\n",
      "Epoch 1/5\n",
      "1267/1267 [==============================] - 5s 4ms/step - loss: 10.4716 - pred_percentage: 0.6472\n",
      "Epoch 2/5\n",
      "1267/1267 [==============================] - 3s 3ms/step - loss: 9.6477 - pred_percentage: 0.7024\n",
      "Epoch 3/5\n",
      "1267/1267 [==============================] - 3s 2ms/step - loss: 9.5280 - pred_percentage: 0.7048\n",
      "Epoch 4/5\n",
      "1267/1267 [==============================] - 3s 2ms/step - loss: 9.4629 - pred_percentage: 0.7088\n",
      "Epoch 5/5\n",
      "1267/1267 [==============================] - 3s 2ms/step - loss: 9.3563 - pred_percentage: 0.7119\n",
      "159/159 [==============================] - 1s 4ms/step\n",
      "Loss was: 9.093047359829429 and accuracy was: 0.6666666666666666\n",
      "Running fold: 2\n",
      "Epoch 1/5\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 10.5679 - pred_percentage: 0.6426\n",
      "Epoch 2/5\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 9.7974 - pred_percentage: 0.6952\n",
      "Epoch 3/5\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 9.6044 - pred_percentage: 0.6921\n",
      "Epoch 4/5\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 9.6120 - pred_percentage: 0.6850A: 0s - loss: 9.9140 - p\n",
      "Epoch 5/5\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 9.5321 - pred_percentage: 0.7007\n",
      "153/153 [==============================] - 1s 8ms/step\n",
      "Loss was: 8.597993508711749 and accuracy was: 0.7189542483660131\n",
      "Running fold: 3\n",
      "Epoch 1/5\n",
      "1268/1268 [==============================] - 4s 3ms/step - loss: 10.2351 - pred_percentage: 0.6412\n",
      "Epoch 2/5\n",
      "1268/1268 [==============================] - 3s 2ms/step - loss: 9.5005 - pred_percentage: 0.6964\n",
      "Epoch 3/5\n",
      "1268/1268 [==============================] - 4s 3ms/step - loss: 9.3033 - pred_percentage: 0.7106A: 0s - loss: 9.2755 - pred_percentage: 0\n",
      "Epoch 4/5\n",
      "1268/1268 [==============================] - 3s 2ms/step - loss: 9.2848 - pred_percentage: 0.7019\n",
      "Epoch 5/5\n",
      "1268/1268 [==============================] - 3s 2ms/step - loss: 9.2829 - pred_percentage: 0.7090\n",
      "158/158 [==============================] - 1s 5ms/step\n",
      "Loss was: 10.103317923183683 and accuracy was: 0.7215189873417721\n",
      "Running fold: 4\n",
      "Epoch 1/5\n",
      "1273/1273 [==============================] - 5s 4ms/step - loss: 10.2126 - pred_percentage: 0.6512\n",
      "Epoch 2/5\n",
      "1273/1273 [==============================] - 3s 2ms/step - loss: 9.4163 - pred_percentage: 0.7093\n",
      "Epoch 3/5\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 9.4246 - pred_percentage: 0.6952\n",
      "Epoch 4/5\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 9.3291 - pred_percentage: 0.6968\n",
      "Epoch 5/5\n",
      "1273/1273 [==============================] - 4s 3ms/step - loss: 9.3488 - pred_percentage: 0.6968\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "Loss was: 9.613758764966251 and accuracy was: 0.7777777777777778\n",
      "Running fold: 5\n",
      "Epoch 1/5\n",
      "1283/1283 [==============================] - 4s 3ms/step - loss: 10.1960 - pred_percentage: 0.6703\n",
      "Epoch 2/5\n",
      "1283/1283 [==============================] - 4s 3ms/step - loss: 9.5831 - pred_percentage: 0.6906\n",
      "Epoch 3/5\n",
      "1283/1283 [==============================] - 4s 3ms/step - loss: 9.4633 - pred_percentage: 0.7030\n",
      "Epoch 4/5\n",
      "1283/1283 [==============================] - 4s 3ms/step - loss: 9.3542 - pred_percentage: 0.7077\n",
      "Epoch 5/5\n",
      "1283/1283 [==============================] - 3s 2ms/step - loss: 9.2451 - pred_percentage: 0.7147\n",
      "143/143 [==============================] - 1s 5ms/step\n",
      "Loss was: 9.161805195616676 and accuracy was: 0.6993006993006993\n",
      "Running fold: 6\n",
      "Epoch 1/5\n",
      "1289/1289 [==============================] - 5s 4ms/step - loss: 10.4828 - pred_percentage: 0.6447\n",
      "Epoch 2/5\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 9.5959 - pred_percentage: 0.7005\n",
      "Epoch 3/5\n",
      "1289/1289 [==============================] - 5s 4ms/step - loss: 9.4876 - pred_percentage: 0.7036\n",
      "Epoch 4/5\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 9.4518 - pred_percentage: 0.7021\n",
      "Epoch 5/5\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 9.3736 - pred_percentage: 0.7013A: 1s\n",
      "137/137 [==============================] - 1s 9ms/step\n",
      "Loss was: 9.000069025224143 and accuracy was: 0.6861313868613139\n",
      "Running fold: 7\n",
      "Epoch 1/5\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 10.3534 - pred_percentage: 0.6478\n",
      "Epoch 2/5\n",
      "1289/1289 [==============================] - 4s 3ms/step - loss: 9.5350 - pred_percentage: 0.6936\n",
      "Epoch 3/5\n",
      "1289/1289 [==============================] - 3s 3ms/step - loss: 9.5071 - pred_percentage: 0.6912\n",
      "Epoch 4/5\n",
      "1289/1289 [==============================] - 3s 2ms/step - loss: 9.3679 - pred_percentage: 0.7075\n",
      "Epoch 5/5\n",
      "1289/1289 [==============================] - 3s 2ms/step - loss: 9.3722 - pred_percentage: 0.7013\n",
      "137/137 [==============================] - 1s 6ms/step\n",
      "Loss was: 10.051630956521869 and accuracy was: 0.656934306569343\n",
      "Running fold: 8\n",
      "Epoch 1/5\n",
      "1298/1298 [==============================] - 4s 3ms/step - loss: 10.1063 - pred_percentage: 0.6903\n",
      "Epoch 2/5\n",
      "1298/1298 [==============================] - 3s 2ms/step - loss: 9.5690 - pred_percentage: 0.7111\n",
      "Epoch 3/5\n",
      "1298/1298 [==============================] - 4s 3ms/step - loss: 9.4641 - pred_percentage: 0.7149\n",
      "Epoch 4/5\n",
      "1298/1298 [==============================] - 4s 3ms/step - loss: 9.3796 - pred_percentage: 0.7119A: - ETA: 0s - loss: 9.3357 - pred_percentage: 0.\n",
      "Epoch 5/5\n",
      "1298/1298 [==============================] - 4s 3ms/step - loss: 9.3594 - pred_percentage: 0.7072\n",
      "128/128 [==============================] - ETA:  - 1s 9ms/step\n",
      "Loss was: 8.59832076006569 and accuracy was: 0.7109375\n",
      "Running fold: 9\n",
      "Epoch 1/5\n",
      "1294/1294 [==============================] - 6s 5ms/step - loss: 10.3328 - pred_percentage: 0.6453\n",
      "Epoch 2/5\n",
      "1294/1294 [==============================] - 5s 4ms/step - loss: 9.6486 - pred_percentage: 0.6932\n",
      "Epoch 3/5\n",
      "1294/1294 [==============================] - 3s 2ms/step - loss: 9.4243 - pred_percentage: 0.7110\n",
      "Epoch 4/5\n",
      "1294/1294 [==============================] - 3s 2ms/step - loss: 9.4549 - pred_percentage: 0.7040\n",
      "Epoch 5/5\n",
      "1294/1294 [==============================] - 3s 2ms/step - loss: 9.3748 - pred_percentage: 0.6971\n",
      "132/132 [==============================] - 1s 8ms/step\n",
      "Loss was: 9.317664009321367 and accuracy was: 0.7121212121212122\n",
      "Running fold: 10\n",
      "Epoch 1/5\n",
      "1300/1300 [==============================] - 8s 6ms/step - loss: 10.6278 - pred_percentage: 0.6177\n",
      "Epoch 2/5\n",
      "1300/1300 [==============================] - 6s 4ms/step - loss: 9.5855 - pred_percentage: 0.7038A: 0s - loss: 9.6742 - pred_pe\n",
      "Epoch 3/5\n",
      "1300/1300 [==============================] - 6s 5ms/step - loss: 9.4862 - pred_percentage: 0.7000- - ETA: 3s - loss: 9.3663 - pre - ETA: 3s -\n",
      "Epoch 4/5\n",
      "1300/1300 [==============================] - 4s 3ms/step - loss: 9.3629 - pred_percentage: 0.7108\n",
      "Epoch 5/5\n",
      "1300/1300 [==============================] - 3s 2ms/step - loss: 9.4272 - pred_percentage: 0.6962\n",
      "126/126 [==============================] - 1s 7ms/step\n",
      "Loss was: 9.367108521716935 and accuracy was: 0.7063492063492064\n",
      "Average prediction accuracy: 0.7056691991354004\n"
     ]
    }
   ],
   "source": [
    "# display(one_minus_two)\n",
    "team_feature_length = len(imp_attributes)\n",
    "\n",
    "n_folds = 10\n",
    "epochs = 5\n",
    "folds = StratifiedKFold(one_minus_two, n_folds = n_folds, shuffle = True)\n",
    "# folds = StratifiedKFold(n_folds = n_folds, shuffle=True)\n",
    "\n",
    "percentages = []\n",
    "for i, (train, test) in enumerate(folds):\n",
    "    print(\"Running fold: {}\".format(i+1))\n",
    "    model = None\n",
    "    model = make_reg_model(team_feature_length)\n",
    "    percentage = train_and_evaluate(model,\n",
    "                       [new_team_ones.iloc[train], new_team_twos.iloc[train]],\n",
    "                       one_minus_two.iloc[train], \n",
    "                       [new_team_ones.iloc[test], new_team_twos.iloc[test]],\n",
    "                       one_minus_two.iloc[test], \n",
    "                       epochs = epochs)\n",
    "    percentages.append(percentage)\n",
    "\n",
    "print(\"Average prediction accuracy: {}\".format(np.mean(percentages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created and tested 3 different models:\n",
    "1. Simple binary classifier. \n",
    "    - Uses our definition of matchups (team_1 - team_2), and outputs 1 if team_1 wins, 0 if team_2 wins.\n",
    "\n",
    "2. Simple Regression.\n",
    "    - Uses our definition of matchups, and tries to predict the difference between points of team_1 and team_2.\n",
    "    - We then use a scoring function (return 1 if score_diff > 0, and 0 otherwise) in order to generate predictions.\n",
    "\n",
    "3. More Complex Regression.\n",
    "    - We feed just the team vectors into the model, and let the model learn an effective definition for what a matchup is.\n",
    "    - The model then outputs a prediction of the score differential (team_1_score - team_2_score).\n",
    "    - We then use the scoring function defined above to generate predictions.\n",
    "    \n",
    "    \n",
    "\n",
    "Now we will use these models (and the training data) in order to generate predictions for the matchups between 2011-2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matchup_testing_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Function to generate the training/testing data.\n",
    "    Based on team_vector_store and the tourney detailed results.\n",
    "    If classification = True, generates for binary classification.\n",
    "    If classification = False, generates for regression.\n",
    "    \n",
    "    Returns training_data\n",
    "    \"\"\"\n",
    "    col_names = list(team_vector_store[2003])\n",
    "\n",
    "    testing_matchup_data = pd.DataFrame(columns = col_names)\n",
    "    matchups = []\n",
    "    \n",
    "    for year in [2011, 2012, 2013]:\n",
    "        season_data = team_vector_store[year].sort_index()\n",
    "        for i1, row1 in season_data.iterrows():\n",
    "            for i2, row2 in season_data.iterrows():\n",
    "#                 print(row2)\n",
    "                if i2 > i1:\n",
    "                    matchups.append([year, i1, i2])\n",
    "                    matchup = row1 - row2\n",
    "                    testing_matchup_data = testing_matchup_data.append([matchup], ignore_index = True)\n",
    "\n",
    "    return testing_matchup_data, matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_data, matchups = generate_matchup_testing_data(team_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_to_csv(model, X_train, Y_train, outfile, testing_data, matchups, classification = True):\n",
    "    model.fit(X_train, Y_train, epochs = 5, batch_size = 1)\n",
    "    if classification:\n",
    "        predictions = model.predict_classes(testing_data)\n",
    "    else:\n",
    "        predictions = model.predict(testing_data)\n",
    "        \n",
    "    result = []\n",
    "    for i in range(len(matchups)):\n",
    "        representation = '_'.join([str(matchups[i][0]), str(matchups[i][1]), str(matchups[i][2])])\n",
    "        if classification:\n",
    "            representation = [representation] + [str(predictions[i][0])]\n",
    "        else:\n",
    "            pred = 1 if predictions[i][0] > 0 else 0\n",
    "            representation = [representation] + [str(pred)]\n",
    "        result.append(representation)\n",
    "\n",
    "    # print(result)\n",
    "    result = pd.DataFrame(data = result, columns = ['game_ID', 'Prediction'])\n",
    "    result.to_csv(outfile, index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_131 (Dense)            (None, 26)                364       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 1)                 27        \n",
      "=================================================================\n",
      "Total params: 1,795\n",
      "Trainable params: 1,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1426/1426 [==============================] - 5s 4ms/step - loss: 0.6288 - acc: 0.7006\n",
      "Epoch 2/5\n",
      "1426/1426 [==============================] - 5s 3ms/step - loss: 0.5896 - acc: 0.7139\n",
      "Epoch 3/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 0.5816 - acc: 0.7202\n",
      "Epoch 4/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 0.5766 - acc: 0.7167\n",
      "Epoch 5/5\n",
      "1426/1426 [==============================] - 4s 3ms/step - loss: 0.5692 - acc: 0.7111\n"
     ]
    }
   ],
   "source": [
    "X_train_class = train_matchup_data_class.drop(['class'], axis = 1)\n",
    "input_shape = X_train_class.shape\n",
    "Y_train_class = train_matchup_data_class['class']\n",
    "binary_class_model = build_keras_model(input_shape, \n",
    "                                       num_units = 2*len(imp_attributes), \n",
    "                                       num_layers = 3, \n",
    "                                       classification = True)\n",
    "\n",
    "# plot_model(binary_class_model, to_file = \"binary_class_model.png\")\n",
    "print(binary_class_model.summary())\n",
    "\n",
    "generate_predictions_to_csv(binary_class_model, \n",
    "                            X_train_class, \n",
    "                            Y_train_class, \n",
    "                            \"bin_predictions.csv\",\n",
    "                            matchup_data,\n",
    "                            matchups,\n",
    "                            classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_135 (Dense)            (None, 26)                364       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1)                 27        \n",
      "=================================================================\n",
      "Total params: 1,795\n",
      "Trainable params: 1,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1426/1426 [==============================] - 5s 3ms/step - loss: 9.8620 - pred_percentage: 0.6802\n",
      "Epoch 2/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.1565 - pred_percentage: 0.7104\n",
      "Epoch 3/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.2001 - pred_percentage: 0.7139\n",
      "Epoch 4/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.0752 - pred_percentage: 0.7202\n",
      "Epoch 5/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.0017 - pred_percentage: 0.7146\n"
     ]
    }
   ],
   "source": [
    "X_train_reg = train_matchup_data_reg.drop(['class'], axis = 1)\n",
    "input_shape = X_train_reg.shape\n",
    "Y_train_reg = train_matchup_data_reg['class']\n",
    "\n",
    "simple_regression_model = build_keras_model(input_shape, \n",
    "                                            num_units = 2*len(imp_attributes), \n",
    "                                            num_layers = 3, \n",
    "                                            classification = False)\n",
    "print(simple_regression_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "generate_predictions_to_csv(simple_regression_model, \n",
    "                            X_train_reg, \n",
    "                            Y_train_reg, \n",
    "                            \"simple_reg_predictions.csv\",\n",
    "                            matchup_data,\n",
    "                            matchups,\n",
    "                            classification = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indiv_testing_data(team_vector_store):\n",
    "    \"\"\"\n",
    "    Function to generate the testing data.\n",
    "    \"\"\"\n",
    "    col_names = list(team_vector_store[2003])\n",
    "\n",
    "    team_ones = pd.DataFrame(columns = col_names)\n",
    "    team_twos = pd.DataFrame(columns = col_names)\n",
    "    matchups = []\n",
    "    \n",
    "    for year in [2011, 2012, 2013]:\n",
    "        season_data = team_vector_store[year].sort_index()\n",
    "        for i1, row1 in season_data.iterrows():\n",
    "            for i2, row2 in season_data.iterrows():\n",
    "#                 print(row2)\n",
    "                if i2 > i1:\n",
    "                    team_ones = team_ones.append(row1, ignore_index = True)\n",
    "                    team_twos = team_twos.append(row2, ignore_index = True)\n",
    "                    matchups.append([year, i1, i2])\n",
    "                    \n",
    "    return [team_ones, team_twos], matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 13)           182         input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 26)           0           dense_139[0][0]                  \n",
      "                                                                 dense_139[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 26)           0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 26)           702         dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, 26)           702         dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_142 (Dense)               (None, 26)           702         dense_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_143 (Dense)               (None, 1)            27          dense_142[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,315\n",
      "Trainable params: 2,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1426/1426 [==============================] - 6s 4ms/step - loss: 10.1844 - pred_percentage: 0.6753\n",
      "Epoch 2/5\n",
      "1426/1426 [==============================] - 5s 3ms/step - loss: 9.5580 - pred_percentage: 0.7069A: 0s - loss: 9.5488 - pred_percentage: 0.706\n",
      "Epoch 3/5\n",
      "1426/1426 [==============================] - 5s 4ms/step - loss: 9.4806 - pred_percentage: 0.7090\n",
      "Epoch 4/5\n",
      "1426/1426 [==============================] - 4s 3ms/step - loss: 9.3818 - pred_percentage: 0.7090\n",
      "Epoch 5/5\n",
      "1426/1426 [==============================] - 3s 2ms/step - loss: 9.3614 - pred_percentage: 0.7069\n"
     ]
    }
   ],
   "source": [
    "X_train = [new_team_ones, new_team_twos]\n",
    "team_dim = len(imp_attributes)\n",
    "Y_train = one_minus_two\n",
    "\n",
    "complex_regression_model = make_reg_model(team_dim)\n",
    "indiv_testing_data, matchups = generate_indiv_testing_data(team_vector_store)\n",
    "\n",
    "print(complex_regression_model.summary())\n",
    "\n",
    "\n",
    "generate_predictions_to_csv(complex_regression_model,\n",
    "                           X_train,\n",
    "                           Y_train,\n",
    "                           \"complex_reg_predictions.csv\",\n",
    "                           indiv_testing_data,\n",
    "                           matchups,\n",
    "                           classification = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n",
      " 2017]\n"
     ]
    }
   ],
   "source": [
    "print(reg_season_years)\n",
    "year = 2017\n",
    "m = team_vector_store[year]\n",
    "# display(m.loc[1274])\n",
    "# display(m.loc[1199])\n",
    "matchup_1 = (m.loc[1274] - m.loc[1199])\n",
    "matchup_1['class'] = 1\n",
    "# display(matchup_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
